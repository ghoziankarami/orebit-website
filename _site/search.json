[
  {
    "objectID": "posts/Spatial Statistics/index.html",
    "href": "posts/Spatial Statistics/index.html",
    "title": "Part 2: Spatial & Statistical Analysis",
    "section": "",
    "text": "With validated data from Part 1, we now explore where the data is located and what it tells us statistically. This post covers:\n\nPillar 2: Spatial Distribution Analysis\nPillar 3: Statistical Characterization\n\nThese pillars help us understand sampling density, identify spatial patterns, and characterize grade distributions - all critical for informed estimation decisions."
  },
  {
    "objectID": "posts/Spatial Statistics/index.html#introduction",
    "href": "posts/Spatial Statistics/index.html#introduction",
    "title": "Part 2: Spatial & Statistical Analysis",
    "section": "",
    "text": "With validated data from Part 1, we now explore where the data is located and what it tells us statistically. This post covers:\n\nPillar 2: Spatial Distribution Analysis\nPillar 3: Statistical Characterization\n\nThese pillars help us understand sampling density, identify spatial patterns, and characterize grade distributions - all critical for informed estimation decisions."
  },
  {
    "objectID": "posts/Spatial Statistics/index.html#setup-and-data-loading",
    "href": "posts/Spatial Statistics/index.html#setup-and-data-loading",
    "title": "Part 2: Spatial & Statistical Analysis",
    "section": "Setup and Data Loading",
    "text": "Setup and Data Loading\n\n\nKode\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(DT)\nlibrary(RColorBrewer)\nlibrary(patchwork)\n\n# Create simulated drilling data\nset.seed(123)\nn_holes &lt;- 50\n\ncollar &lt;- data.frame(\n  hole_id = paste0(\"DDH\", sprintf(\"%03d\", 1:n_holes)),\n  x = runif(n_holes, 500000, 501000),\n  y = runif(n_holes, 9000000, 9001000),\n  rl = runif(n_holes, 100, 200)\n)\n\n# Assay data with spatial correlation\nassay_list &lt;- lapply(1:n_holes, function(i) {\n  n_intervals &lt;- sample(15:25, 1)\n  depths &lt;- seq(0, by = 2, length.out = n_intervals)\n  \n  # Add spatial correlation to grades\n  distance_from_center &lt;- sqrt((collar$x[i] - 500500)^2 + (collar$y[i] - 9000500)^2)\n  grade_factor &lt;- exp(-distance_from_center / 300)\n  \n  data.frame(\n    hole_id = collar$hole_id[i],\n    from = depths[-length(depths)],\n    to = depths[-1],\n    au_ppm = pmax(0, rnorm(n_intervals - 1, mean = 1.5 * grade_factor, sd = 2)),\n    ag_ppm = pmax(0, rnorm(n_intervals - 1, mean = 15 * grade_factor, sd = 20)),\n    cu_pct = pmax(0, rnorm(n_intervals - 1, mean = 0.5 * grade_factor, sd = 0.8))\n  )\n})\nassay &lt;- do.call(rbind, assay_list)\n\n# Lithology data\nlitho_codes &lt;- c(\"Andesite\", \"Diorite\", \"Mineralized_Zone\", \"Altered_Volcanics\")\nlithology_list &lt;- lapply(collar$hole_id, function(hid) {\n  n_litho &lt;- sample(4:8, 1)\n  depths &lt;- sort(c(0, sample(5:40, n_litho - 1), 50))\n  \n  data.frame(\n    hole_id = hid,\n    from = depths[-length(depths)],\n    to = depths[-1],\n    lithology = sample(litho_codes, n_litho, replace = TRUE, \n                      prob = c(0.3, 0.2, 0.3, 0.2))\n  )\n})\nlithology &lt;- do.call(rbind, lithology_list)\n\n# Clean and prepare data\ncollar &lt;- collar %&gt;% janitor::clean_names() %&gt;%\n  select(hole_id, x, y, z = rl) %&gt;%\n  mutate(hole_id = as.character(hole_id))\n\nassay &lt;- assay %&gt;% janitor::clean_names() %&gt;%\n  mutate(hole_id = as.character(hole_id))\n\nlithology &lt;- lithology %&gt;% janitor::clean_names() %&gt;%\n  select(hole_id, from, to, lithology) %&gt;%\n  mutate(hole_id = as.character(hole_id))\n\n# Merge datasets\ncollar_std &lt;- collar\nassay_std &lt;- assay\nlithology_std &lt;- lithology\n\ncombined_data &lt;- assay_std %&gt;%\n  left_join(collar_std, by = \"hole_id\") %&gt;%\n  mutate(mid_point = from + (to - from) / 2) %&gt;%\n  left_join(\n    lithology_std %&gt;% rename(litho_from = from, litho_to = to),\n    by = join_by(hole_id, between(mid_point, litho_from, litho_to))\n  ) %&gt;%\n  select(-mid_point, -litho_from, -litho_to)"
  },
  {
    "objectID": "posts/Spatial Statistics/index.html#objective",
    "href": "posts/Spatial Statistics/index.html#objective",
    "title": "Part 2: Spatial & Statistical Analysis",
    "section": "Objective",
    "text": "Objective\nUnderstand where your data is located in 3D space:\n\nAre drillholes evenly distributed?\nWhere are the dense vs sparse areas?\nWhat are the initial mineralization trends?\nIs there clustering or bias in sampling?\n\n\n\n\n\n\n\nTipWhy Spatial Analysis Matters\n\n\n\nSpatial patterns directly impact:\n\nKriging neighborhood selection\nSearch ellipse orientation\nEstimation variance\nClassification confidence (Measured, Indicated, Inferred)"
  },
  {
    "objectID": "posts/Spatial Statistics/index.html#d-drillhole-location-map",
    "href": "posts/Spatial Statistics/index.html#d-drillhole-location-map",
    "title": "Part 2: Spatial & Statistical Analysis",
    "section": "2D Drillhole Location Map",
    "text": "2D Drillhole Location Map\nLet’s start with a plan view showing collar locations.\n\nBasic Collar Map\n\n\nKode\n# Create 2D plan view\np_2d &lt;- ggplot(collar_std, aes(x = x, y = y)) +\n  geom_point(size = 3, alpha = 0.7, color = \"steelblue\") +\n  geom_text(aes(label = hole_id), \n            hjust = -0.2, vjust = 0.5, \n            size = 2.5, \n            check_overlap = TRUE) +\n  coord_equal() +\n  labs(\n    title = \"2D Drillhole Plan View\",\n    subtitle = paste(nrow(collar_std), \"drillholes\"),\n    x = \"Easting (m)\",\n    y = \"Northing (m)\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    panel.grid.minor = element_blank()\n  )\n\np_2d\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCatatanSpatial Pattern Observations\n\n\n\nLook for:\n\nClustering: Groups of closely spaced holes\nGaps: Areas with no drilling\nGrid pattern: Regular vs irregular spacing\nDirectional bias: Preferential drilling directions\n\n\n\n\n\nInteractive 2D Map with Average Grades\n\n\nKode\n# Calculate average grade per hole (example: using au_ppm)\navg_grades &lt;- combined_data %&gt;%\n  group_by(hole_id, x, y) %&gt;%\n  summarise(\n    avg_au = mean(au_ppm, na.rm = TRUE),\n    max_au = max(au_ppm, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  filter(!is.na(avg_au))\n\n# Create interactive plotly map\nplot_ly(data = avg_grades,\n        x = ~x, y = ~y,\n        type = 'scatter', mode = 'markers',\n        marker = list(\n          size = 10,\n          color = ~avg_au,\n          colorscale = 'Viridis',\n          showscale = TRUE,\n          colorbar = list(title = \"Avg Au&lt;br&gt;(ppm)\")\n        ),\n        text = ~paste0(\n          \"Hole: \", hole_id, \"&lt;br&gt;\",\n          \"Avg Au: \", round(avg_au, 3), \" ppm&lt;br&gt;\",\n          \"Max Au: \", round(max_au, 3), \" ppm\"\n        ),\n        hoverinfo = 'text') %&gt;%\n  layout(\n    title = \"2D Drillhole Map: Average Gold Grades\",\n    xaxis = list(title = \"Easting (m)\"),\n    yaxis = list(\n      title = \"Northing (m)\",\n      scaleanchor = \"x\",\n      scaleratio = 1\n    )\n  )\n\n\n\n\n\n\n\n\nSampling Density Analysis\n\n\nKode\n# Calculate local sampling density\ndensity_estimate &lt;- MASS::kde2d(\n  collar_std$x, \n  collar_std$y, \n  n = 50\n)\n\n# Convert to data frame for ggplot\ndensity_df &lt;- expand.grid(\n  x = density_estimate$x,\n  y = density_estimate$y\n) %&gt;%\n  mutate(density = as.vector(density_estimate$z))\n\np_density &lt;- ggplot() +\n  geom_tile(data = density_df, aes(x = x, y = y, fill = density)) +\n  scale_fill_viridis_c(option = \"plasma\", name = \"Density\") +\n  geom_point(data = collar_std, aes(x = x, y = y), \n             size = 2, color = \"white\", alpha = 0.6) +\n  coord_equal() +\n  labs(\n    title = \"Sampling Density Heatmap\",\n    subtitle = \"Darker areas indicate higher drillhole density\",\n    x = \"Easting (m)\",\n    y = \"Northing (m)\"\n  ) +\n  theme_minimal()\n\np_density\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPentingClassification Implications\n\n\n\nSampling density directly impacts resource classification:\n\nDense areas: Higher confidence → Measured/Indicated\nSparse areas: Lower confidence → Indicated/Inferred\nNo data areas: Extrapolation risk"
  },
  {
    "objectID": "posts/Spatial Statistics/index.html#composite-length-analysis",
    "href": "posts/Spatial Statistics/index.html#composite-length-analysis",
    "title": "Part 2: Spatial & Statistical Analysis",
    "section": "Composite Length Analysis",
    "text": "Composite Length Analysis\nUnderstanding sample support is critical for geostatistics.\n\n\nKode\n# Calculate composite lengths\ncombined_data &lt;- combined_data %&gt;%\n  mutate(composite_length = to - from)\n\n# Summary statistics\nlength_summary &lt;- combined_data %&gt;%\n  summarise(\n    `Min Length` = min(composite_length, na.rm = TRUE),\n    `Q1` = quantile(composite_length, 0.25, na.rm = TRUE),\n    `Median` = median(composite_length, na.rm = TRUE),\n    `Mean` = mean(composite_length, na.rm = TRUE),\n    `Q3` = quantile(composite_length, 0.75, na.rm = TRUE),\n    `Max Length` = max(composite_length, na.rm = TRUE),\n    `Std Dev` = sd(composite_length, na.rm = TRUE)\n  )\n\ndatatable(length_summary,\n          caption = \"Table 1: Composite Length Statistics (meters)\",\n          options = list(dom = 't')) %&gt;%\n  formatRound(columns = 1:7, digits = 2)\n\n\n\n\n\n\nKode\n# Histogram\np_length &lt;- ggplot(combined_data, aes(x = composite_length)) +\n  geom_histogram(bins = 30, fill = \"skyblue\", color = \"black\", alpha = 0.7) +\n  geom_vline(xintercept = mean(combined_data$composite_length, na.rm = TRUE),\n             color = \"red\", linetype = \"dashed\", size = 1) +\n  annotate(\"text\", \n           x = mean(combined_data$composite_length, na.rm = TRUE),\n           y = Inf,\n           label = paste(\"Mean:\", round(mean(combined_data$composite_length, na.rm = TRUE), 2), \"m\"),\n           vjust = 2, color = \"red\", fontface = \"bold\") +\n  labs(\n    title = \"Composite Length Distribution\",\n    subtitle = \"Check for consistency in sample support\",\n    x = \"Composite Length (m)\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal()\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nKode\np_length\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPeringatanVariable Sample Support Issues\n\n\n\nInconsistent sample lengths can cause:\n\nSupport effect: Grade variance differences\nBias in estimation: Longer samples overweighted\nVariogram artifacts: False spatial continuity\n\nSolution: Composite to a consistent length (typically 1m or 2m)"
  },
  {
    "objectID": "posts/Spatial Statistics/index.html#d-visualization",
    "href": "posts/Spatial Statistics/index.html#d-visualization",
    "title": "Part 2: Spatial & Statistical Analysis",
    "section": "3D Visualization",
    "text": "3D Visualization\nUnderstanding the vertical distribution of data is essential.\n\n\nKode\n# Prepare 3D data with depth\ndata_3d &lt;- combined_data %&gt;%\n  mutate(z_sample = z - ((from + to) / 2)) %&gt;%\n  filter(!is.na(x) & !is.na(y) & !is.na(z_sample) & !is.na(au_ppm))\n\n# Sample data for performance (use 50% of data)\nset.seed(123)\ndata_3d_sample &lt;- sample_frac(data_3d, 0.5)\n\n# Create 3D scatter plot\nplot_ly(\n  data = data_3d_sample,\n  x = ~x, y = ~y, z = ~z_sample,\n  type = 'scatter3d', mode = 'markers',\n  marker = list(\n    color = ~au_ppm,\n    colorscale = 'Viridis',\n    colorbar = list(title = \"Au (ppm)\"),\n    size = 3\n  ),\n  text = ~paste0(\n    \"Hole: \", hole_id, \"&lt;br&gt;\",\n    \"Depth: \", round(from, 1), \"-\", round(to, 1), \"m&lt;br&gt;\",\n    \"Au: \", round(au_ppm, 3), \" ppm\"\n  ),\n  hoverinfo = 'text'\n) %&gt;%\n  layout(\n    title = \"3D Drillhole Assay View\",\n    scene = list(\n      xaxis = list(title = \"Easting\"),\n      yaxis = list(title = \"Northing\"),\n      zaxis = list(title = \"Elevation (RL)\", autorange = \"reversed\"),\n      aspectratio = list(x = 1, y = 1, z = 0.5)\n    )\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nTip3D Visualization Benefits\n\n\n\n3D views help identify:\n\nVertical trends in mineralization\nDip and plunge of ore bodies\nDrilling depth coverage\nSpatial clustering in 3D space"
  },
  {
    "objectID": "posts/Spatial Statistics/index.html#objective-1",
    "href": "posts/Spatial Statistics/index.html#objective-1",
    "title": "Part 2: Spatial & Statistical Analysis",
    "section": "Objective",
    "text": "Objective\nUnderstand the “personality” of your grade data:\n\nWhat does the distribution look like?\nAre there outliers?\nSingle or multiple populations?\nWhat top-cut value is appropriate?"
  },
  {
    "objectID": "posts/Spatial Statistics/index.html#univariate-statistics",
    "href": "posts/Spatial Statistics/index.html#univariate-statistics",
    "title": "Part 2: Spatial & Statistical Analysis",
    "section": "Univariate Statistics",
    "text": "Univariate Statistics\n\nSummary Statistics by Grade Variable\n\n\nKode\n# Select numeric grade columns\ngrade_cols &lt;- c(\"au_ppm\", \"ag_ppm\", \"cu_pct\")\n\n# Calculate comprehensive statistics\nstats_summary &lt;- combined_data %&gt;%\n  select(all_of(grade_cols)) %&gt;%\n  pivot_longer(everything(), names_to = \"Grade\", values_to = \"Value\") %&gt;%\n  group_by(Grade) %&gt;%\n  summarise(\n    Count = sum(!is.na(Value)),\n    Mean = mean(Value, na.rm = TRUE),\n    Median = median(Value, na.rm = TRUE),\n    `Std Dev` = sd(Value, na.rm = TRUE),\n    CV = sd(Value, na.rm = TRUE) / mean(Value, na.rm = TRUE),\n    Min = min(Value, na.rm = TRUE),\n    Q1 = quantile(Value, 0.25, na.rm = TRUE),\n    Q3 = quantile(Value, 0.75, na.rm = TRUE),\n    Max = max(Value, na.rm = TRUE),\n    Skewness = e1071::skewness(Value, na.rm = TRUE)\n  ) %&gt;%\n  mutate(across(where(is.numeric), ~round(.x, 3)))\n\ndatatable(stats_summary,\n          caption = \"Table 2: Comprehensive Grade Statistics\",\n          options = list(scrollX = TRUE, pageLength = 10))\n\n\n\n\n\n\n\n\n\n\n\n\nCatatanKey Statistical Indicators\n\n\n\n\nCV (Coefficient of Variation): Measure of variability (CV &gt; 1 indicates high variability)\nSkewness: Positive skew common in grade data (long right tail)\nMean vs Median: Large differences suggest outliers"
  },
  {
    "objectID": "posts/Spatial Statistics/index.html#grade-distribution-analysis",
    "href": "posts/Spatial Statistics/index.html#grade-distribution-analysis",
    "title": "Part 2: Spatial & Statistical Analysis",
    "section": "Grade Distribution Analysis",
    "text": "Grade Distribution Analysis\n\nHistograms\n\n\nKode\n# Create histograms for each grade\nplot_list &lt;- lapply(grade_cols, function(col) {\n  ggplot(combined_data, aes(x = .data[[col]])) +\n    geom_histogram(bins = 50, fill = \"skyblue\", color = \"black\", alpha = 0.7) +\n    geom_vline(xintercept = mean(combined_data[[col]], na.rm = TRUE),\n               color = \"red\", linetype = \"dashed\", size = 1) +\n    geom_vline(xintercept = median(combined_data[[col]], na.rm = TRUE),\n               color = \"blue\", linetype = \"dashed\", size = 1) +\n    labs(\n      title = paste(\"Distribution of\", col),\n      x = col,\n      y = \"Frequency\"\n    ) +\n    theme_minimal() +\n    theme(plot.title = element_text(size = 10))\n})\n\n# Combine plots\n(plot_list[[1]] | plot_list[[2]]) / plot_list[[3]]\n\n\n\n\n\n\n\n\n\n\n\nLog-Normal Probability Plots\nEssential for identifying populations and assessing normality.\n\n\nKode\n# Create Q-Q plots for log-transformed data\nqq_plots &lt;- lapply(grade_cols, function(col) {\n  data_subset &lt;- combined_data %&gt;%\n    filter(!is.na(.data[[col]]) & .data[[col]] &gt; 0)\n  \n  ggplot(data_subset, aes(sample = log(.data[[col]]))) +\n    stat_qq(color = \"steelblue\", alpha = 0.5) +\n    stat_qq_line(color = \"red\", size = 1) +\n    labs(\n      title = paste(\"Log-Normal Q-Q Plot:\", col),\n      x = \"Theoretical Quantiles\",\n      y = \"Sample Quantiles (log scale)\"\n    ) +\n    theme_minimal()\n})\n\n(qq_plots[[1]] | qq_plots[[2]]) / qq_plots[[3]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPentingPopulation Identification\n\n\n\nBreaks or changes in slope on Q-Q plots indicate:\n\nMultiple populations: Different geological/mineralization domains\nOutliers: High-grade samples requiring investigation\nMixed distributions: Need for domain separation\n\nAction: Use this information for domaining decisions!"
  },
  {
    "objectID": "posts/Spatial Statistics/index.html#outlier-detection-and-analysis",
    "href": "posts/Spatial Statistics/index.html#outlier-detection-and-analysis",
    "title": "Part 2: Spatial & Statistical Analysis",
    "section": "Outlier Detection and Analysis",
    "text": "Outlier Detection and Analysis\n\nBoxplots by Lithology\n\n\nKode\n# Focus on Au for outlier analysis\np_box &lt;- ggplot(combined_data, aes(x = lithology, y = au_ppm, fill = lithology)) +\n  geom_boxplot(outlier.colour = \"red\", outlier.shape = 16, outlier.size = 2) +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(\n    title = \"Gold Grade Distribution by Lithology\",\n    subtitle = \"Red points indicate statistical outliers (&gt;1.5 IQR)\",\n    x = \"Lithology\",\n    y = \"Au (ppm)\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.position = \"none\"\n  )\n\np_box\n\n\n\n\n\n\n\n\n\n\n\nOutlier Table\n\n\nKode\n# Calculate IQR-based outliers\noutlier_threshold &lt;- combined_data %&gt;%\n  summarise(\n    Q1 = quantile(au_ppm, 0.25, na.rm = TRUE),\n    Q3 = quantile(au_ppm, 0.75, na.rm = TRUE)\n  ) %&gt;%\n  mutate(\n    IQR = Q3 - Q1,\n    lower_bound = Q1 - 1.5 * IQR,\n    upper_bound = Q3 + 1.5 * IQR\n  )\n\noutliers &lt;- combined_data %&gt;%\n  filter(au_ppm &gt; outlier_threshold$upper_bound | \n         au_ppm &lt; outlier_threshold$lower_bound) %&gt;%\n  select(hole_id, from, to, lithology, au_ppm) %&gt;%\n  arrange(desc(au_ppm))\n\ndatatable(outliers,\n          caption = paste(\"Table 3: Outlier Samples (n =\", nrow(outliers), \")\"),\n          options = list(pageLength = 10, scrollX = TRUE)) %&gt;%\n  formatStyle('au_ppm', backgroundColor = '#ffebee', fontWeight = 'bold')\n\n\n\n\n\n\n\n\n\n\n\n\nPeringatanOutlier Treatment Options\n\n\n\n\nKeep as-is: If geologically justified\nTop-cut (cap): Limit maximum grade\nRemove: If analytical errors suspected\nInvestigate: Check for sample preparation issues\n\nNever remove outliers without geological justification!"
  },
  {
    "objectID": "posts/Spatial Statistics/index.html#top-cut-analysis",
    "href": "posts/Spatial Statistics/index.html#top-cut-analysis",
    "title": "Part 2: Spatial & Statistical Analysis",
    "section": "Top-Cut Analysis",
    "text": "Top-Cut Analysis\nDetermining appropriate grade capping values.\n\n\nKode\n# Calculate percentiles\npercentiles &lt;- seq(0.9, 1.0, by = 0.01)\npercentile_values &lt;- quantile(combined_data$au_ppm, probs = percentiles, na.rm = TRUE)\n\npercentile_df &lt;- data.frame(\n  Percentile = percentiles * 100,\n  Grade = percentile_values\n)\n\n# Plot grade vs percentile\np_percentile &lt;- ggplot(percentile_df, aes(x = Percentile, y = Grade)) +\n  geom_line(color = \"steelblue\", size = 1.5) +\n  geom_point(size = 2, color = \"darkblue\") +\n  geom_vline(xintercept = c(95, 97.5, 99), \n             linetype = \"dashed\", \n             color = c(\"orange\", \"red\", \"darkred\")) +\n  annotate(\"text\", x = 95, y = max(percentile_values) * 0.9, \n           label = \"P95\", color = \"orange\") +\n  annotate(\"text\", x = 97.5, y = max(percentile_values) * 0.9, \n           label = \"P97.5\", color = \"red\") +\n  annotate(\"text\", x = 99, y = max(percentile_values) * 0.9, \n           label = \"P99\", color = \"darkred\") +\n  labs(\n    title = \"Top-Cut Analysis: Grade vs Percentile\",\n    subtitle = \"Common thresholds: P95, P97.5, P99\",\n    x = \"Percentile (%)\",\n    y = \"Au Grade (ppm)\"\n  ) +\n  theme_minimal()\n\np_percentile\n\n\n\n\n\n\n\n\n\n\nImpact of Top-Cutting\n\n\nKode\n# Compare statistics with different top-cuts\ntop_cuts &lt;- c(Inf, \n              quantile(combined_data$au_ppm, 0.95, na.rm = TRUE),\n              quantile(combined_data$au_ppm, 0.975, na.rm = TRUE),\n              quantile(combined_data$au_ppm, 0.99, na.rm = TRUE))\n\nimpact_df &lt;- data.frame(\n  Scenario = c(\"No Top-Cut\", \"P95 Cut\", \"P97.5 Cut\", \"P99 Cut\"),\n  `Cut Value` = round(top_cuts, 2),\n  Mean = sapply(top_cuts, function(tc) {\n    mean(pmin(combined_data$au_ppm, tc), na.rm = TRUE)\n  }),\n  Median = sapply(top_cuts, function(tc) {\n    median(pmin(combined_data$au_ppm, tc), na.rm = TRUE)\n  }),\n  `Std Dev` = sapply(top_cuts, function(tc) {\n    sd(pmin(combined_data$au_ppm, tc), na.rm = TRUE)\n  }),\n  CV = sapply(top_cuts, function(tc) {\n    sd(pmin(combined_data$au_ppm, tc), na.rm = TRUE) / \n      mean(pmin(combined_data$au_ppm, tc), na.rm = TRUE)\n  })\n) %&gt;%\n  mutate(across(where(is.numeric), ~round(.x, 3)))\n\ndatatable(impact_df,\n          caption = \"Table 4: Impact of Top-Cutting on Statistics\",\n          options = list(dom = 't'))\n\n\n\n\n\n\n\n\n\n\n\n\nTipSelecting Top-Cut Values\n\n\n\nConsider:\n\nStatistical analysis: Probability plots, percentiles\nGeological context: Are high grades real or analytical errors?\nImpact on resources: Balance between grade and tonnage\nIndustry standards: P95-P99 common for precious metals\nDomaining: Different top-cuts for different domains\n\nDocument your decision with geological and statistical justification!"
  },
  {
    "objectID": "posts/Spatial Statistics/index.html#grade-distribution-by-lithology",
    "href": "posts/Spatial Statistics/index.html#grade-distribution-by-lithology",
    "title": "Part 2: Spatial & Statistical Analysis",
    "section": "Grade Distribution by Lithology",
    "text": "Grade Distribution by Lithology\nUnderstanding how grades vary by rock type.\n\n\nKode\n# Faceted histograms by lithology\nggplot(combined_data, aes(x = au_ppm)) +\n  geom_histogram(bins = 30, fill = \"skyblue\", color = \"black\", alpha = 0.7) +\n  facet_wrap(~ lithology, scales = \"free_y\", ncol = 2) +\n  labs(\n    title = \"Gold Grade Distribution by Lithology\",\n    x = \"Au (ppm)\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal() +\n  theme(strip.text = element_text(face = \"bold\", size = 10))\n\n\n\n\n\n\n\n\n\n\nStatistical Comparison by Lithology\n\n\nKode\nlitho_stats &lt;- combined_data %&gt;%\n  group_by(lithology) %&gt;%\n  summarise(\n    Count = n(),\n    Mean = mean(au_ppm, na.rm = TRUE),\n    Median = median(au_ppm, na.rm = TRUE),\n    `Std Dev` = sd(au_ppm, na.rm = TRUE),\n    CV = sd(au_ppm, na.rm = TRUE) / mean(au_ppm, na.rm = TRUE),\n    Max = max(au_ppm, na.rm = TRUE)\n  ) %&gt;%\n  arrange(desc(Mean)) %&gt;%\n  mutate(across(where(is.numeric) & !Count, ~round(.x, 3)))\n\ndatatable(litho_stats,\n          caption = \"Table 5: Grade Statistics by Lithology\",\n          options = list(pageLength = 10)) %&gt;%\n  formatStyle(\n    'Mean',\n    background = styleColorBar(litho_stats$Mean, 'lightblue'),\n    backgroundSize = '100% 90%',\n    backgroundRepeat = 'no-repeat',\n    backgroundPosition = 'center'\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nCatatanGeological Insights\n\n\n\nThis analysis reveals:\n\nHost rocks: Which lithologies contain mineralization?\nBarren zones: Low-grade lithologies to exclude?\nGrade differences: Justification for separate domains?\nContinuity: Are grades consistent within lithology?"
  },
  {
    "objectID": "posts/Spatial Statistics/index.html#summary-pillars-2-3",
    "href": "posts/Spatial Statistics/index.html#summary-pillars-2-3",
    "title": "Part 2: Spatial & Statistical Analysis",
    "section": "Summary: Pillars 2 & 3",
    "text": "Summary: Pillars 2 & 3\n\nKey Achievements\nSpatial Analysis (Pillar 2): ✓ Visualized drillhole distribution in 2D and 3D ✓ Identified sampling density patterns ✓ Analyzed composite length consistency ✓ Recognized spatial trends in mineralization\nStatistical Analysis (Pillar 3): ✓ Characterized grade distributions ✓ Identified outliers and proposed top-cuts ✓ Compared statistics by lithology ✓ Assessed data populations\n\n\nChecklist: Before Moving to Pillar 4\n\nSpatial patterns understood and documented\nSampling density adequate for classification\nGrade populations identified\nOutliers investigated and treatment decided\nTop-cut values proposed with justification\nLithology-grade relationships documented\n\n\n\n\n\n\n\nTipReady for Geological Controls?\n\n\n\nWith spatial and statistical understanding complete, you’re ready to connect these patterns to geology in Part 3: Geological Controls & Domain Definition."
  },
  {
    "objectID": "posts/Spatial Statistics/index.html#best-practices-recap",
    "href": "posts/Spatial Statistics/index.html#best-practices-recap",
    "title": "Part 2: Spatial & Statistical Analysis",
    "section": "Best Practices Recap",
    "text": "Best Practices Recap\n\nCommon Pitfalls to Avoid\n\nIgnoring spatial patterns - Statistics alone aren’t enough\nOver-relying on automation - Always apply geological thinking\nArbitrary top-cutting - Document geological justification\nSkipping lithology analysis - Rock types control grades\n\n\n\nDocumentation Requirements\nFor JORC compliance, document:\n\nSpatial distribution maps and density analysis\nStatistical summary by domain/lithology\nOutlier treatment decisions with justification\nTop-cut analysis and selected values\nPopulation identification methodology\n\n\nPrevious: ← Part 1 - Data Validation\nNext: Part 3 - Geological Controls & Domain Definition →"
  },
  {
    "objectID": "posts/Data Validation/index.html",
    "href": "posts/Data Validation/index.html",
    "title": "Part 1: Foundation & Data Validation",
    "section": "",
    "text": "Before diving into the technical details, let’s address a fundamental question: Why do mining projects fail?\nThe answer often lies in the quality of the foundational data. Research shows that up to 70% of errors in resource estimation stem from inadequate data validation and EDA processes.\n\n\nGarbage In, Garbage Out (GIGO) - This principle is fundamental to all data analysis work. Poor quality data will always produce poor models, leading to:\n\nInaccurate resource estimates\nFailed mine plans\nInvestor confidence loss\nRegulatory non-compliance\nMillions in losses\n\n\n\n\n\n\n\nPentingIndustry Reality\n\n\n\nIn mining, we don’t get second chances. The quality of our EDA determines whether we build a mine or lose millions in poor decisions."
  },
  {
    "objectID": "posts/Data Validation/index.html#why-eda-is-not-optional",
    "href": "posts/Data Validation/index.html#why-eda-is-not-optional",
    "title": "Part 1: Foundation & Data Validation",
    "section": "",
    "text": "Before diving into the technical details, let’s address a fundamental question: Why do mining projects fail?\nThe answer often lies in the quality of the foundational data. Research shows that up to 70% of errors in resource estimation stem from inadequate data validation and EDA processes.\n\n\nGarbage In, Garbage Out (GIGO) - This principle is fundamental to all data analysis work. Poor quality data will always produce poor models, leading to:\n\nInaccurate resource estimates\nFailed mine plans\nInvestor confidence loss\nRegulatory non-compliance\nMillions in losses\n\n\n\n\n\n\n\nPentingIndustry Reality\n\n\n\nIn mining, we don’t get second chances. The quality of our EDA determines whether we build a mine or lose millions in poor decisions."
  },
  {
    "objectID": "posts/Data Validation/index.html#eda-as-an-industry-standard",
    "href": "posts/Data Validation/index.html#eda-as-an-industry-standard",
    "title": "Part 1: Foundation & Data Validation",
    "section": "EDA as an Industry Standard",
    "text": "EDA as an Industry Standard\nEDA is not just “best practice” - it’s a mandatory requirement for professional resource estimation.\n\nJORC Code Compliance\nThe JORC Code requires that all resource reports be:\n\nTransparent: Methods and data quality must be clearly documented\nMaterial: All relevant information affecting value must be disclosed\nCompetent: Prepared by qualified professionals\n\nEDA directly supports these requirements by:\n\nDocumenting data quality and limitations\nIdentifying material data issues\nProviding evidence for geological interpretations\n\n\n\nCompetent Person Responsibilities\nAs a Competent Person (CP), thorough EDA is part of your due diligence. You are responsible for:\n\nVerifying data integrity\nDocumenting QA/QC procedures\nEnsuring estimation assumptions are data-supported\nDefending your resource model to auditors and regulators"
  },
  {
    "objectID": "posts/Data Validation/index.html#the-4-pillar-eda-framework",
    "href": "posts/Data Validation/index.html#the-4-pillar-eda-framework",
    "title": "Part 1: Foundation & Data Validation",
    "section": "The 4 Pillar EDA Framework",
    "text": "The 4 Pillar EDA Framework\nThis series follows a systematic 4-pillar approach to EDA:\n\n\n\n\n\n\nEach pillar builds upon the previous, creating a comprehensive understanding of your dataset."
  },
  {
    "objectID": "posts/Data Validation/index.html#pillar-1-data-validation-integrity",
    "href": "posts/Data Validation/index.html#pillar-1-data-validation-integrity",
    "title": "Part 1: Foundation & Data Validation",
    "section": "Pillar 1: Data Validation & Integrity",
    "text": "Pillar 1: Data Validation & Integrity\nThe first pillar is the foundation of all subsequent analysis. Without proper data validation, all downstream work becomes meaningless.\n\nWhat We Check\nA comprehensive data validation workflow includes:\n\nFile Integration Checks\n\nCollar file completeness\nAssay file completeness\nLithology file completeness\nCross-file consistency\n\nMissing Data Detection\n\nCollars without assay data\nAssays without collar coordinates\nLithology gaps\n\nInterval Validation\n\nOverlapping intervals\nGaps in sampling\nDepth consistency\n\nGeometric Validation\n\nData above/below topography\nSurvey data quality\nCoordinate system consistency\n\n\n\n\n\n\n\n\nPeringatanCritical Principle\n\n\n\nOne bad data point can invalidate an entire block model if not caught early. Data integrity issues compound through every step of the modeling process."
  },
  {
    "objectID": "posts/Data Validation/index.html#practical-implementation-with-geodataviz",
    "href": "posts/Data Validation/index.html#practical-implementation-with-geodataviz",
    "title": "Part 1: Foundation & Data Validation",
    "section": "Practical Implementation with GeoDataViz",
    "text": "Practical Implementation with GeoDataViz\nLet’s see how these validation checks are implemented using real drilling data.\n\nStep 1: Load Required Libraries\n\n\nKode\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(DT)\nlibrary(plotly)\nlibrary(janitor)\n\n\n\n\nStep 2: Create Sample Data\nFor this demonstration, we’ll create simulated drilling data that mimics real geological scenarios:\n\n\nKode\n# Create simulated collar data\nset.seed(123)\nn_holes &lt;- 50\n\ncollar &lt;- data.frame(\n  hole_id = paste0(\"DDH\", sprintf(\"%03d\", 1:n_holes)),\n  x = runif(n_holes, 500000, 501000),\n  y = runif(n_holes, 9000000, 9001000),\n  rl = runif(n_holes, 100, 200)\n)\n\n# Create simulated assay data (multiple intervals per hole)\nassay_list &lt;- lapply(collar$hole_id, function(hid) {\n  n_intervals &lt;- sample(15:25, 1)\n  depths &lt;- seq(0, by = 2, length.out = n_intervals)\n  \n  data.frame(\n    hole_id = hid,\n    from = depths[-length(depths)],\n    to = depths[-1],\n    au_ppm = pmax(0, rnorm(n_intervals - 1, mean = 1.5, sd = 2)),\n    ag_ppm = pmax(0, rnorm(n_intervals - 1, mean = 15, sd = 20)),\n    cu_pct = pmax(0, rnorm(n_intervals - 1, mean = 0.5, sd = 0.8))\n  )\n})\nassay &lt;- do.call(rbind, assay_list)\n\n# Create simulated lithology data\nlitho_codes &lt;- c(\"Andesite\", \"Diorite\", \"Mineralized_Zone\", \"Altered_Volcanics\")\nlithology_list &lt;- lapply(collar$hole_id, function(hid) {\n  n_litho &lt;- sample(4:8, 1)\n  depths &lt;- sort(c(0, sample(5:40, n_litho - 1), 50))\n  \n  data.frame(\n    hole_id = hid,\n    from = depths[-length(depths)],\n    to = depths[-1],\n    lithology = sample(litho_codes, n_litho, replace = TRUE)\n  )\n})\nlithology &lt;- do.call(rbind, lithology_list)\n\n# Clean names\ncollar &lt;- janitor::clean_names(collar)\nassay &lt;- janitor::clean_names(assay)\nlithology &lt;- janitor::clean_names(lithology)\n\n# Display structure\ncat(\"Collar records:\", nrow(collar), \"\\n\")\n\n\nCollar records: 50 \n\n\nKode\ncat(\"Assay records:\", nrow(assay), \"\\n\")\n\n\nAssay records: 981 \n\n\nKode\ncat(\"Lithology records:\", nrow(lithology), \"\\n\")\n\n\nLithology records: 309 \n\n\n\n\n\n\n\n\nCatatanAbout the Sample Data\n\n\n\nThis is simulated data designed to demonstrate EDA workflows. In practice, you would load your own drilling data from CSV files or databases.\n\n\n\n\nStep 3: File Record Count Validation\nThe first check: do we have data in all files?\n\n\nKode\nfile_counts &lt;- data.frame(\n  File = c(\"Collar\", \"Assay\", \"Lithology\"),\n  Records = c(nrow(collar), nrow(assay), nrow(lithology))\n)\n\ndatatable(file_counts, \n          options = list(dom = 't'),\n          caption = \"Table 1: File Record Counts\")\n\n\n\n\n\n\n\n\n\n\n\n\nTipInterpretation\n\n\n\nAll three files should have records. Empty files indicate data loading issues that must be resolved before proceeding.\n\n\n\n\nStep 4: Cross-File Consistency Checks\n\nCheck 1: Collars Missing Assay Data\n\n\nKode\n# Identify collars without assay data\nmissing_assay &lt;- anti_join(\n  collar %&gt;% distinct(hole_id),\n  assay %&gt;% distinct(hole_id),\n  by = \"hole_id\"\n)\n\nif(nrow(missing_assay) &gt; 0) {\n  datatable(missing_assay,\n            caption = \"Table 2: Collars Missing Assay Data\",\n            options = list(pageLength = 5))\n} else {\n  cat(\"✓ All collars have corresponding assay data.\\n\")\n}\n\n\n✓ All collars have corresponding assay data.\n\n\n\n\nCheck 2: Assays Missing Collar Data\n\n\nKode\n# Identify assays without collar coordinates\nmissing_collar &lt;- anti_join(\n  assay %&gt;% distinct(hole_id),\n  collar %&gt;% distinct(hole_id),\n  by = \"hole_id\"\n)\n\nif(nrow(missing_collar) &gt; 0) {\n  datatable(missing_collar,\n            caption = \"Table 3: Assays Missing Collar Data\",\n            options = list(pageLength = 5))\n} else {\n  cat(\"✓ All assays have corresponding collar coordinates.\\n\")\n}\n\n\n✓ All assays have corresponding collar coordinates.\n\n\n\n\n\n\n\n\nCatatanCommon Causes\n\n\n\nMismatches often result from:\n\nTypos in hole IDs (e.g., “DDH001” vs “DDH-001”)\nIncomplete data transfers\nHoles logged but not yet assayed\nData entry errors\n\n\n\n\n\n\nStep 5: Interval Validation\nOne of the most critical checks: ensuring assay intervals are continuous without gaps or overlaps.\n\n\nKode\n# Check for interval errors (gaps/overlaps)\ninterval_errors &lt;- assay %&gt;%\n  arrange(hole_id, from) %&gt;%\n  group_by(hole_id) %&gt;%\n  mutate(\n    prev_to = lag(to),\n    has_error = !is.na(prev_to) & (from != prev_to)\n  ) %&gt;%\n  ungroup() %&gt;%\n  filter(has_error) %&gt;%\n  select(hole_id, prev_to, from, to)\n\nif(nrow(interval_errors) &gt; 0) {\n  datatable(interval_errors,\n            caption = \"Table 4: Interval Errors (Gaps/Overlaps)\",\n            options = list(pageLength = 10, scrollX = TRUE)) %&gt;%\n    formatStyle('from', backgroundColor = '#ffebee') %&gt;%\n    formatStyle('prev_to', backgroundColor = '#fff9c4')\n} else {\n  cat(\"✓ No interval gaps or overlaps detected.\\n\")\n}\n\n\n✓ No interval gaps or overlaps detected.\n\n\n\n\n\n\n\n\nPentingWhy This Matters\n\n\n\nInterval errors can cause:\n\nIncorrect composite calculations\nGrade dilution or concentration artifacts\nInaccurate tonnage estimates\nBiased variography\n\n\n\n\n\nVisualization: Interval Error Example\n\n\nKode\n# Create example visualization if errors exist\nif(nrow(interval_errors) &gt; 0) {\n  # Take first hole with errors as example\n  example_hole &lt;- interval_errors$hole_id[1]\n  example_data &lt;- assay %&gt;%\n    filter(hole_id == example_hole) %&gt;%\n    arrange(from) %&gt;%\n    head(10)\n  \n  ggplot(example_data, aes(y = from, yend = to)) +\n    geom_segment(aes(x = 0, xend = 1), size = 8, color = \"steelblue\", alpha = 0.7) +\n    geom_text(aes(x = 0.5, y = (from + to)/2, label = paste0(from, \"-\", to)), \n              color = \"white\", fontface = \"bold\", size = 3) +\n    scale_y_reverse() +\n    coord_flip() +\n    labs(\n      title = paste(\"Interval Visualization:\", example_hole),\n      subtitle = \"Look for gaps (white space) or overlaps (segments touching)\",\n      x = NULL,\n      y = \"Depth (m)\"\n    ) +\n    theme_minimal() +\n    theme(\n      axis.text.y = element_blank(),\n      axis.ticks.y = element_blank(),\n      panel.grid.major.y = element_blank()\n    )\n}"
  },
  {
    "objectID": "posts/Data Validation/index.html#data-validation-summary",
    "href": "posts/Data Validation/index.html#data-validation-summary",
    "title": "Part 1: Foundation & Data Validation",
    "section": "Data Validation Summary",
    "text": "Data Validation Summary\n\nKey Metrics Dashboard\n\n\nKode\n# Create validation summary\nvalidation_summary &lt;- data.frame(\n  Check = c(\n    \"Total Collars\",\n    \"Total Assay Intervals\",\n    \"Total Lithology Intervals\",\n    \"Collars Missing Assays\",\n    \"Assays Missing Collars\",\n    \"Interval Errors\"\n  ),\n  Count = c(\n    nrow(collar),\n    nrow(assay),\n    nrow(lithology),\n    nrow(missing_assay),\n    nrow(missing_collar),\n    nrow(interval_errors)\n  ),\n  Status = c(\n    \"✓\", \"✓\", \"✓\",\n    ifelse(nrow(missing_assay) == 0, \"✓\", \"⚠\"),\n    ifelse(nrow(missing_collar) == 0, \"✓\", \"⚠\"),\n    ifelse(nrow(interval_errors) == 0, \"✓\", \"⚠\")\n  )\n)\n\ndatatable(validation_summary,\n          options = list(dom = 't', ordering = FALSE),\n          caption = \"Table 5: Data Validation Summary\",\n          rownames = FALSE) %&gt;%\n  formatStyle(\n    'Status',\n    color = styleEqual(c('✓', '⚠'), c('green', 'orange')),\n    fontWeight = 'bold'\n  )"
  },
  {
    "objectID": "posts/Data Validation/index.html#best-practices-for-data-validation",
    "href": "posts/Data Validation/index.html#best-practices-for-data-validation",
    "title": "Part 1: Foundation & Data Validation",
    "section": "Best Practices for Data Validation",
    "text": "Best Practices for Data Validation\n\nDocumentation Requirements\nFor JORC compliance, document:\n\nData Sources\n\nWho collected the data?\nWhen was it collected?\nWhat QA/QC protocols were followed in the field?\n\nValidation Process\n\nWhat checks were performed?\nWhat issues were found?\nHow were issues resolved?\n\nData Limitations\n\nKnown gaps or uncertainties\nData quality issues that couldn’t be resolved\nImpact on estimation confidence\n\n\n\n\nCommon Pitfalls to Avoid\n\n\n\n\n\n\nPeringatanDon’t Skip These Steps\n\n\n\n\nRushing validation to meet deadlines - Always leads to problems later\nAssuming data is clean - Always validate, even from trusted sources\nFixing issues without documentation - Record all changes for audit trail\nIgnoring “small” errors - Small errors compound in complex workflows"
  },
  {
    "objectID": "posts/Data Validation/index.html#integration-and-data-merging",
    "href": "posts/Data Validation/index.html#integration-and-data-merging",
    "title": "Part 1: Foundation & Data Validation",
    "section": "Integration and Data Merging",
    "text": "Integration and Data Merging\nOnce validation is complete, we can safely merge our datasets:\n\n\nKode\n# Standardize column names\ncollar_std &lt;- collar %&gt;%\n  select(hole_id, x = x, y = y, z = rl) %&gt;%\n  mutate(hole_id = as.character(hole_id))\n\nassay_std &lt;- assay %&gt;%\n  select(hole_id, from, to, everything()) %&gt;%\n  mutate(hole_id = as.character(hole_id))\n\nlithology_std &lt;- lithology %&gt;%\n  select(hole_id, from, to, lithology) %&gt;%\n  mutate(hole_id = as.character(hole_id))\n\n# Merge data\ncombined_data &lt;- assay_std %&gt;%\n  left_join(collar_std, by = \"hole_id\") %&gt;%\n  mutate(mid_point = from + (to - from) / 2) %&gt;%\n  left_join(\n    lithology_std %&gt;% rename(litho_from = from, litho_to = to),\n    by = join_by(hole_id, between(mid_point, litho_from, litho_to))\n  ) %&gt;%\n  select(-mid_point, -litho_from, -litho_to)\n\ncat(\"Combined dataset rows:\", nrow(combined_data), \"\\n\")\n\n\nCombined dataset rows: 1090 \n\n\nKode\ncat(\"Columns:\", paste(names(combined_data), collapse = \", \"), \"\\n\")\n\n\nColumns: hole_id, from, to, au_ppm, ag_ppm, cu_pct, x, y, z, lithology \n\n\n\nPreview Combined Data\n\n\nKode\ndatatable(head(combined_data, 50),\n          options = list(\n            pageLength = 10,\n            scrollX = TRUE,\n            scrollY = \"400px\"\n          ),\n          caption = \"Table 6: Combined Dataset Preview\") %&gt;%\n  formatRound(columns = c('from', 'to', 'x', 'y', 'z'), digits = 2)"
  },
  {
    "objectID": "posts/Data Validation/index.html#checklist-before-moving-to-pillar-2",
    "href": "posts/Data Validation/index.html#checklist-before-moving-to-pillar-2",
    "title": "Part 1: Foundation & Data Validation",
    "section": "Checklist: Before Moving to Pillar 2",
    "text": "Checklist: Before Moving to Pillar 2\nBefore proceeding to spatial analysis, ensure:\n\nAll data files loaded successfully\nNo unexpected missing collars or assays\nInterval errors identified and resolved\nData merge completed without warnings\nValidation results documented\nKnown limitations noted\n\n\n\n\n\n\n\nTipReady for the Next Step?\n\n\n\nWith clean, validated data, you’re ready to explore spatial patterns in Part 2: Spatial & Statistical Analysis."
  },
  {
    "objectID": "posts/Data Validation/index.html#summary",
    "href": "posts/Data Validation/index.html#summary",
    "title": "Part 1: Foundation & Data Validation",
    "section": "Summary",
    "text": "Summary\nData validation is the foundation of reliable resource estimation. Key takeaways:\n\nNever skip validation - It’s mandatory for JORC compliance\nCheck everything - Files, intervals, cross-references\nDocument thoroughly - Create audit trails\nFix issues early - Problems compound downstream\nValidate assumptions - Don’t trust data blindly\n\nRemember the GIGO principle: Quality data is the only path to quality models."
  },
  {
    "objectID": "posts/Data Validation/index.html#tools-and-resources",
    "href": "posts/Data Validation/index.html#tools-and-resources",
    "title": "Part 1: Foundation & Data Validation",
    "section": "Tools and Resources",
    "text": "Tools and Resources\n\nGeoDataViz: GitHub Repository\nJORC Code: 2012 Edition\nContact: ghoziankarami@gmail.com\n\n\nNext: Part 2 - Spatial & Statistical Analysis →"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Technical Resources",
    "section": "",
    "text": "The Orebit Methodology Hub\n\n\nWelcome to our knowledge base, structured around the Four Pillars of Effective Exploratory Data Analysis (EDA). Here you’ll find in-depth articles and practical tutorials demonstrating our practitioner-built approach to Data Validation, Univariate/Bivariate Analysis, Spatial Analysis, and Reporting. Master the workflows that power our tools and transform your data into reliable intelligence.\n\n\n\n\n\n\n\n\n   \n    \n    \n      Urut berdasar\n      Default\n      \n        Judul\n      \n      \n        Tanggal - Paling lama\n      \n      \n        Tanggal - Paling baru\n      \n      \n        Penulis\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nPart 3: Geological Controls & Domain Definition\n\n\n\nEDA\n\nGeological Domains\n\nDomaining\n\nResource Estimation\n\n\n\n\n\n\n\nGhozian Islam Karami\n\n\n29 Jan 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nPart 2: Spatial & Statistical Analysis\n\n\n\nEDA\n\nSpatial Analysis\n\nStatistics\n\nVisualization\n\n\n\n\n\n\n\nGhozian Islam Karami\n\n\n22 Jan 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nPart 1: Foundation & Data Validation\n\n\n\nEDA\n\nData Validation\n\nQAQC\n\nResource Estimation\n\n\n\n\n\n\n\nGhozian Islam Karami\n\n\n15 Jan 2025\n\n\n\n\n\n\nTidak ada yang cocok"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Orebit",
    "section": "",
    "text": "Orebit Geological Solutions develops professional R Shiny applications that transform how geoscientists and mining engineers approach data analysis. Our mission is to provide cost-effective, transparent, and powerful analytical tools that rival expensive proprietary software—without the prohibitive licensing fees."
  },
  {
    "objectID": "about.html#advanced-geological-solutions-for-the-modern-mining-industry",
    "href": "about.html#advanced-geological-solutions-for-the-modern-mining-industry",
    "title": "About Orebit",
    "section": "",
    "text": "Orebit Geological Solutions develops professional R Shiny applications that transform how geoscientists and mining engineers approach data analysis. Our mission is to provide cost-effective, transparent, and powerful analytical tools that rival expensive proprietary software—without the prohibitive licensing fees."
  },
  {
    "objectID": "about.html#the-challenge-we-address",
    "href": "about.html#the-challenge-we-address",
    "title": "About Orebit",
    "section": "The Challenge We Address",
    "text": "The Challenge We Address\nThe mining industry faces a critical paradox: geological analysis demands increasingly sophisticated tools, yet traditional software solutions create barriers through:\n\nHigh Costs: Enterprise geological software can exceed $50,000 per year in licensing fees\nVendor Lock-in: Proprietary systems limit flexibility and customization\nLimited Transparency: Black-box algorithms make validation and reproducibility difficult\nAccessibility Gaps: Small companies and consultants struggle to afford industry-standard tools\n\nOrebit exists to eliminate these barriers."
  },
  {
    "objectID": "about.html#our-vision-mission",
    "href": "about.html#our-vision-mission",
    "title": "About Orebit",
    "section": "Our Vision & Mission",
    "text": "Our Vision & Mission\n\nVision\nTo be the catalyst for a data-driven transformation in geoscience, establishing open and transparent tools as the standard for every professional to unlock the full potential of their data.\n\n\nMission\nTo empower geoscientists and mining engineers by providing practical, practitioner-built workflows and expert guidance, enabling them to confidently transform complex geological data into reliable, actionable intelligence."
  },
  {
    "objectID": "about.html#the-team-behind-orebit",
    "href": "about.html#the-team-behind-orebit",
    "title": "About Orebit",
    "section": "The Team Behind Orebit",
    "text": "The Team Behind Orebit\nOrebit is led by Ghozian Islam Karami, Senior Geologist and Certified BNSP Competent Person (CP) with over a decade of mining industry experience. His expertise in offshore tin exploration, resource evaluation, and geological data analysis ensures that every Orebit application addresses real-world geological challenges.\nCore Expertise: - Geostatistics and resource modeling - R programming and Shiny application development - Quality assurance systems for mining operations - Geospatial analysis and 3D geological modeling\nProfessional Credentials: - Certified Competent Person (CP) - BNSP - M.Sc. Geological Sciences - KFUPM - B.Sc. Geology - Universitas Padjadjaran - Senior Geologist - PERHAPI\nThis combination of geological expertise and technical development skills ensures Orebit applications are both geologically sound and technically sophisticated."
  },
  {
    "objectID": "about.html#contact-information",
    "href": "about.html#contact-information",
    "title": "About Orebit",
    "section": "Contact Information",
    "text": "Contact Information\nEmail: orebit.id@gmail.com GitHub: github.com/ghoziankarami LinkedIn: Ghozian Islam Karami"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "From Raw Ore to Reliable Bits.",
    "section": "",
    "text": "We transform complex geological data into digital clarity. Our “glass box” approach, built on open-source tools, provides the transparent and powerful process to turn your raw data into your most valuable strategic asset.\n\n Launch GeoDataViz Demo  Explore Our Methodology\n\n\n\n\nOur tools and resources are built around a core methodology designed to ensure robust and reliable geological analysis. Each pillar represents a critical stage in transforming data into insight.\n\n\n\n\n\n\n\nEnsuring data integrity from the start. We focus on identifying outliers, errors, and inconsistencies before they impact your model.\n\n\n\n\n\n\n\n\nUnderstanding the character of each variable and the relationships between them. This is the foundation of geological domaining.\n\n\n\n\n\n\n\n\nAnalyzing how your data behaves in 3D space. We investigate trends, continuity, and spatial relationships critical for geostatistics.\n\n\n\n\n\n\n\n\nDelivering clear, reproducible reports that document every step of the analysis, ensuring transparency and auditability.\n\n\n\n\n\n\n\nOur applications are designed to implement the Four Pillars methodology seamlessly, empowering you with cost-effective and transparent tools.\n\n\n\n\n\nAvailable Now\n\n[cite_start]Transform your geological workflow with professional-grade exploratory data analysis and 3D visualization, built specifically for drillhole datasets. [cite: 12] Launch Free Demo →\n\n\n\n\nComing Q2 2026\n\n[cite_start]Professional geostatistical analysis and resource modeling. [cite: 13] Advanced kriging, variogram modeling, and uncertainty quantification. Join Beta List →\n\n\n\n\nComing Q3 2026\n\n[cite_start]Sophisticated mine planning and optimization algorithms. [cite: 14] Maximize NPV while considering operational constraints and environmental factors. Get Notified →\n\n\n\n\n\n\n[cite_start]Stay updated on our upcoming resource estimation and pit optimization tools. [cite: 15]\n\n Get Notified"
  },
  {
    "objectID": "index.html#the-four-pillars-of-effective-eda",
    "href": "index.html#the-four-pillars-of-effective-eda",
    "title": "From Raw Ore to Reliable Bits.",
    "section": "",
    "text": "Our tools and resources are built around a core methodology designed to ensure robust and reliable geological analysis. Each pillar represents a critical stage in transforming data into insight.\n\n\n\n\n\n\n\nEnsuring data integrity from the start. We focus on identifying outliers, errors, and inconsistencies before they impact your model.\n\n\n\n\n\n\n\n\nUnderstanding the character of each variable and the relationships between them. This is the foundation of geological domaining.\n\n\n\n\n\n\n\n\nAnalyzing how your data behaves in 3D space. We investigate trends, continuity, and spatial relationships critical for geostatistics.\n\n\n\n\n\n\n\n\nDelivering clear, reproducible reports that document every step of the analysis, ensuring transparency and auditability."
  },
  {
    "objectID": "posts/Geological Domain/index.html",
    "href": "posts/Geological Domain/index.html",
    "title": "Part 3: Geological Controls & Domain Definition",
    "section": "",
    "text": "We’ve validated our data (Part 1), explored spatial patterns, and characterized statistical distributions (Part 2). Now comes the most critical step: connecting numbers to geology.\nPillar 4: Geological Controls Analysis transforms statistical insights into geologically defensible estimation domains - the backbone of reliable resource models."
  },
  {
    "objectID": "posts/Geological Domain/index.html#introduction",
    "href": "posts/Geological Domain/index.html#introduction",
    "title": "Part 3: Geological Controls & Domain Definition",
    "section": "",
    "text": "We’ve validated our data (Part 1), explored spatial patterns, and characterized statistical distributions (Part 2). Now comes the most critical step: connecting numbers to geology.\nPillar 4: Geological Controls Analysis transforms statistical insights into geologically defensible estimation domains - the backbone of reliable resource models."
  },
  {
    "objectID": "posts/Geological Domain/index.html#the-critical-question",
    "href": "posts/Geological Domain/index.html#the-critical-question",
    "title": "Part 3: Geological Controls & Domain Definition",
    "section": "The Critical Question",
    "text": "The Critical Question\n\n“Where is mineralization located and WHY?”\n\nThis isn’t just about mapping high grades. It’s about understanding the geological controls that govern mineralization:\n\nWhich rock types host mineralization?\nAre there geochemical relationships between elements?\nHow does grade behave at geological contacts?\nWhat structural controls exist?\n\n\n\n\n\n\n\nPentingWhy Geological Domains Matter\n\n\n\nPoor domaining is a leading cause of resource estimation failure:\n\nMixed populations create unrealistic variograms\nInappropriate kriging produces smoothing artifacts\nClassification confidence is compromised\nMining selectively targets become unrealistic\n\nGood domains = Reliable estimates"
  },
  {
    "objectID": "posts/Geological Domain/index.html#setup",
    "href": "posts/Geological Domain/index.html#setup",
    "title": "Part 3: Geological Controls & Domain Definition",
    "section": "Setup",
    "text": "Setup\n\n\nKode\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(DT)\nlibrary(RColorBrewer)\nlibrary(GGally)\nlibrary(patchwork)\n\n# Create simulated drilling data with geological controls\nset.seed(456)\nn_holes &lt;- 50\n\ncollar &lt;- data.frame(\n  hole_id = paste0(\"DDH\", sprintf(\"%03d\", 1:n_holes)),\n  x = runif(n_holes, 500000, 501000),\n  y = runif(n_holes, 9000000, 9001000),\n  rl = runif(n_holes, 100, 200)\n)\n\n# Generate assay data with lithology-controlled grades\nlitho_codes &lt;- c(\"Andesite\", \"Diorite\", \"Mineralized_Zone\", \"Altered_Volcanics\")\nlitho_grade_means &lt;- c(0.5, 0.8, 3.5, 2.0)  # Different grades by lithology\n\nassay_list &lt;- lapply(1:n_holes, function(i) {\n  n_intervals &lt;- sample(15:25, 1)\n  depths &lt;- seq(0, by = 2, length.out = n_intervals)\n  \n  # Assign lithology to each interval\n  litho_idx &lt;- sample(1:length(litho_codes), n_intervals - 1, replace = TRUE,\n                     prob = c(0.3, 0.2, 0.3, 0.2))\n  \n  # Generate grades based on lithology\n  au_grades &lt;- sapply(litho_idx, function(idx) {\n    max(0, rnorm(1, mean = litho_grade_means[idx], sd = 1.5))\n  })\n  \n  # Correlated Ag and Cu\n  ag_grades &lt;- au_grades * runif(n_intervals - 1, 8, 12) + rnorm(n_intervals - 1, 0, 5)\n  cu_grades &lt;- au_grades * 0.3 + rnorm(n_intervals - 1, 0, 0.3)\n  \n  data.frame(\n    hole_id = collar$hole_id[i],\n    from = depths[-length(depths)],\n    to = depths[-1],\n    au_ppm = pmax(0, au_grades),\n    ag_ppm = pmax(0, ag_grades),\n    cu_pct = pmax(0, cu_grades),\n    lithology = litho_codes[litho_idx]\n  )\n})\n\ncombined_data &lt;- do.call(rbind, assay_list) %&gt;%\n  left_join(collar, by = \"hole_id\") %&gt;%\n  janitor::clean_names()\n\n# Prepare standard formats\ncollar_std &lt;- collar %&gt;% janitor::clean_names()\nassay_std &lt;- combined_data %&gt;% select(hole_id, from, to, au_ppm, ag_ppm, cu_pct)\nlithology_std &lt;- combined_data %&gt;% select(hole_id, from, to, lithology)"
  },
  {
    "objectID": "posts/Geological Domain/index.html#part-a-lithology-analysis",
    "href": "posts/Geological Domain/index.html#part-a-lithology-analysis",
    "title": "Part 3: Geological Controls & Domain Definition",
    "section": "Part A: Lithology Analysis",
    "text": "Part A: Lithology Analysis\n\nGrade Distribution by Rock Type\nThe most fundamental control: which rocks contain ore?\n\n\nKode\n# Calculate statistics by lithology\nlitho_summary &lt;- combined_data %&gt;%\n  group_by(lithology) %&gt;%\n  summarise(\n    n = n(),\n    mean_au = mean(au_ppm, na.rm = TRUE),\n    median_au = median(au_ppm, na.rm = TRUE),\n    max_au = max(au_ppm, na.rm = TRUE),\n    sd_au = sd(au_ppm, na.rm = TRUE)\n  ) %&gt;%\n  arrange(desc(mean_au))\n\n# Create color palette\nn_litho &lt;- length(unique(combined_data$lithology))\nlitho_colors &lt;- setNames(\n  brewer.pal(max(3, min(n_litho, 9)), \"Set1\")[1:n_litho],\n  sort(unique(combined_data$lithology))\n)\n\n# Boxplot with statistical annotations\np_litho_box &lt;- ggplot(combined_data, aes(x = reorder(lithology, au_ppm, FUN = median), \n                                          y = au_ppm, \n                                          fill = lithology)) +\n  geom_boxplot(outlier.alpha = 0.3) +\n  stat_summary(fun = mean, geom = \"point\", shape = 23, size = 3, \n               fill = \"red\", color = \"darkred\") +\n  scale_fill_manual(values = litho_colors) +\n  labs(\n    title = \"Gold Grade Distribution by Lithology\",\n    subtitle = \"Box = IQR, Diamond = Mean, Line = Median\",\n    x = \"Lithology (ordered by median grade)\",\n    y = \"Au (ppm)\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.position = \"none\"\n  )\n\np_litho_box\n\n\n\n\n\n\n\n\n\n\n\nStatistical Summary by Lithology\n\n\nKode\ndatatable(litho_summary,\n          caption = \"Table 1: Gold Statistics by Lithology\",\n          options = list(pageLength = 10)) %&gt;%\n  formatRound(columns = c('mean_au', 'median_au', 'max_au', 'sd_au'), digits = 3) %&gt;%\n  formatStyle(\n    'mean_au',\n    background = styleColorBar(litho_summary$mean_au, 'lightblue'),\n    backgroundSize = '100% 90%',\n    backgroundRepeat = 'no-repeat',\n    backgroundPosition = 'center'\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nCatatanGeological Interpretation\n\n\n\nFrom this analysis, identify:\n\nPrimary ore hosts: Lithologies with highest mean/median grades\nBarren waste: Low-grade lithologies to exclude\nGrade variance: High CV suggests mixed mineralization styles\nOutlier behavior: Which rocks have extreme values?\n\nAction: Candidate lithologies for separate estimation domains\n\n\n\n\nGrade-Tonnage Curves by Lithology\nUnderstanding the economic potential of each rock type.\n\n\nKode\n# Calculate grade-tonnage curves\ncutoffs &lt;- seq(0, 5, by = 0.1)\n\ngt_curves &lt;- combined_data %&gt;%\n  expand_grid(cutoff = cutoffs) %&gt;%\n  filter(au_ppm &gt;= cutoff) %&gt;%\n  group_by(lithology, cutoff) %&gt;%\n  summarise(\n    tonnage_pct = n() / nrow(combined_data) * 100,\n    avg_grade = mean(au_ppm, na.rm = TRUE),\n    .groups = 'drop'\n  )\n\n# Plot tonnage vs cutoff\np_tonnage &lt;- ggplot(gt_curves, aes(x = cutoff, y = tonnage_pct, color = lithology)) +\n  geom_line(size = 1.2) +\n  scale_color_manual(values = litho_colors) +\n  labs(\n    title = \"Tonnage vs Cut-off Grade\",\n    x = \"Cut-off Grade (ppm Au)\",\n    y = \"Tonnage (% of total)\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\n# Plot grade vs cutoff\np_grade &lt;- ggplot(gt_curves, aes(x = cutoff, y = avg_grade, color = lithology)) +\n  geom_line(size = 1.2) +\n  scale_color_manual(values = litho_colors) +\n  labs(\n    title = \"Average Grade vs Cut-off\",\n    x = \"Cut-off Grade (ppm Au)\",\n    y = \"Average Grade (ppm)\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\np_tonnage / p_grade"
  },
  {
    "objectID": "posts/Geological Domain/index.html#part-b-element-correlation-analysis",
    "href": "posts/Geological Domain/index.html#part-b-element-correlation-analysis",
    "title": "Part 3: Geological Controls & Domain Definition",
    "section": "Part B: Element Correlation Analysis",
    "text": "Part B: Element Correlation Analysis\n\nBivariate Relationships\nUnderstanding element associations reveals mineralization processes.\n\n\nKode\n# Select grade variables for correlation\ngrade_vars &lt;- c(\"au_ppm\", \"ag_ppm\", \"cu_pct\")\n\n# Create scatter plot matrix\nsuppressWarnings({\n  ggpairs(\n    combined_data %&gt;% select(all_of(grade_vars), lithology),\n    mapping = aes(color = lithology, alpha = 0.5),\n    upper = list(continuous = wrap(\"cor\", size = 3)),\n    lower = list(continuous = wrap(\"points\", alpha = 0.3, size = 0.5)),\n    diag = list(continuous = wrap(\"densityDiag\", alpha = 0.5))\n  ) +\n    scale_color_manual(values = litho_colors) +\n    scale_fill_manual(values = litho_colors) +\n    theme_minimal() +\n    labs(title = \"Element Correlation Matrix by Lithology\")\n})\n\n\n\n\n\n\n\n\n\n\n\nRegression Analysis: Au vs Ag\n\n\nKode\n# Calculate regression by lithology\nregression_data &lt;- combined_data %&gt;%\n  filter(!is.na(au_ppm) & !is.na(ag_ppm)) %&gt;%\n  group_by(lithology) %&gt;%\n  filter(n() &gt; 10) %&gt;%\n  nest() %&gt;%\n  mutate(\n    model = purrr::map(data, ~lm(ag_ppm ~ au_ppm, data = .x)),\n    r_squared = purrr::map_dbl(model, ~summary(.x)$r.squared),\n    slope = purrr::map_dbl(model, ~coef(.x)[2]),\n    intercept = purrr::map_dbl(model, ~coef(.x)[1])\n  )\n\n# Create scatter plot with regression lines\np_regression &lt;- ggplot(combined_data, aes(x = au_ppm, y = ag_ppm)) +\n  geom_point(aes(color = lithology), alpha = 0.5, size = 2) +\n  geom_smooth(aes(color = lithology), method = \"lm\", se = FALSE, size = 1.2) +\n  scale_color_manual(values = litho_colors) +\n  labs(\n    title = \"Gold vs Silver: Correlation by Lithology\",\n    subtitle = \"Different slopes suggest different mineralization styles\",\n    x = \"Au (ppm)\",\n    y = \"Ag (ppm)\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\np_regression\n\n\n\n\n\n\n\n\n\n\n\nCorrelation Summary Table\n\n\nKode\nregression_summary &lt;- regression_data %&gt;%\n  select(lithology, r_squared, slope, intercept) %&gt;%\n  arrange(desc(r_squared)) %&gt;%\n  mutate(\n    equation = paste0(\"Ag = \", round(slope, 2), \" × Au + \", round(intercept, 2)),\n    r_squared = round(r_squared, 3)\n  ) %&gt;%\n  select(lithology, r_squared, equation)\n\ndatatable(regression_summary,\n          caption = \"Table 2: Au-Ag Correlation by Lithology\",\n          options = list(pageLength = 10, dom = 't')) %&gt;%\n  formatStyle(\n    'r_squared',\n    background = styleColorBar(regression_summary$r_squared, 'lightgreen'),\n    backgroundSize = '100% 90%',\n    backgroundRepeat = 'no-repeat',\n    backgroundPosition = 'center'\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nTipGeochemical Insights\n\n\n\nStrong correlations indicate:\n\nCoupled mineralization: Elements deposited together\nCommon sources: Shared ore fluids or processes\nDomain boundaries: Changes in correlation suggest domain breaks\n\nWeak correlations may indicate:\n\nDifferent mineralization events: Overprinting\nRemobilization: Secondary processes\nMixed populations: Need for further domain subdivision"
  },
  {
    "objectID": "posts/Geological Domain/index.html#part-c-contact-analysis",
    "href": "posts/Geological Domain/index.html#part-c-contact-analysis",
    "title": "Part 3: Geological Controls & Domain Definition",
    "section": "Part C: Contact Analysis",
    "text": "Part C: Contact Analysis\n\nGrade Behavior at Geological Boundaries\nHow does grade change across lithology contacts?\n\n\nKode\n# Identify contact zones (simplified approach)\ncontact_zones &lt;- combined_data %&gt;%\n  arrange(hole_id, from) %&gt;%\n  group_by(hole_id) %&gt;%\n  mutate(\n    next_litho = lead(lithology),\n    is_contact = lithology != next_litho & !is.na(next_litho),\n    contact_type = if_else(is_contact, \n                          paste(lithology, \"→\", next_litho), \n                          NA_character_)\n  ) %&gt;%\n  filter(is_contact) %&gt;%\n  ungroup()\n\n# Plot grade at contacts\nif(nrow(contact_zones) &gt; 0) {\n  p_contacts &lt;- ggplot(contact_zones, aes(x = contact_type, y = au_ppm)) +\n    geom_boxplot(fill = \"lightcoral\", alpha = 0.7) +\n    geom_jitter(width = 0.2, alpha = 0.5, size = 2) +\n    labs(\n      title = \"Gold Grades at Lithology Contacts\",\n      subtitle = \"Understanding grade behavior at geological boundaries\",\n      x = \"Contact Type\",\n      y = \"Au (ppm)\"\n    ) +\n    theme_minimal() +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1))\n  \n  print(p_contacts)\n} else {\n  cat(\"No contact zones identified in the dataset.\\n\")\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCatatanContact Zone Interpretation\n\n\n\nSharp grade changes at contacts may indicate:\n\nStructural controls: Faults or shears\nAlteration halos: Gradational vs sharp boundaries\nDomain boundaries: Where to draw estimation domains\nDilution zones: Material to exclude from ore domains"
  },
  {
    "objectID": "posts/Geological Domain/index.html#domain-definition-workflow",
    "href": "posts/Geological Domain/index.html#domain-definition-workflow",
    "title": "Part 3: Geological Controls & Domain Definition",
    "section": "Domain Definition Workflow",
    "text": "Domain Definition Workflow\n\nCombining All Evidence\nNow we synthesize findings from all analyses:\n\n\nKode\n# IMPROVED: More realistic domain classification\n# Step 1: Classify each interval\ndomain_definition &lt;- combined_data %&gt;%\n  mutate(\n    interval_domain = case_when(\n      lithology == \"Mineralized_Zone\" & au_ppm &gt;= 1.5 ~ \"High Grade Ore\",\n      lithology == \"Mineralized_Zone\" & au_ppm &gt;= 0.3 ~ \"Low Grade Ore\",\n      lithology == \"Altered_Volcanics\" & au_ppm &gt;= 0.8 ~ \"Altered Ore\",\n      au_ppm &lt; 0.3 ~ \"Waste\",\n      TRUE ~ \"Transitional\"\n    )\n  )\n\n# Step 2: Summarize by hole (dominant domain approach)\nhole_summary &lt;- domain_definition %&gt;%\n  group_by(hole_id) %&gt;%\n  summarise(\n    n_intervals = n(),\n    avg_au = mean(au_ppm, na.rm = TRUE),\n    max_au = max(au_ppm, na.rm = TRUE),\n    has_high_grade = any(interval_domain == \"High Grade Ore\"),\n    pct_ore = sum(interval_domain %in% c(\"High Grade Ore\", \"Low Grade Ore\", \"Altered Ore\")) / n() * 100,\n    .groups = 'drop'\n  )\n\n# Step 3: Final hole-level domain classification\nhole_domains &lt;- hole_summary %&gt;%\n  mutate(\n    hole_domain = case_when(\n      has_high_grade & avg_au &gt;= 1.0 ~ \"High Grade Ore\",\n      avg_au &gt;= 0.8 ~ \"Low Grade Ore\",\n      avg_au &gt;= 0.5 | pct_ore &gt;= 30 ~ \"Altered Ore\",\n      avg_au &gt;= 0.3 ~ \"Transitional\",\n      TRUE ~ \"Waste\"\n    )\n  )\n\n# Summary by domain\ndomain_summary &lt;- domain_definition %&gt;%\n  group_by(interval_domain) %&gt;%\n  summarise(\n    Intervals = n(),\n    `Mean Au` = mean(au_ppm, na.rm = TRUE),\n    `Median Au` = median(au_ppm, na.rm = TRUE),\n    `Mean Ag` = mean(ag_ppm, na.rm = TRUE),\n    `Mean Cu` = mean(cu_pct, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  arrange(desc(`Mean Au`)) %&gt;%\n  mutate(across(where(is.numeric) & !Intervals, ~round(.x, 3)))\n\ndatatable(domain_summary,\n          caption = \"Table 3: Proposed Estimation Domains (Interval-Based)\",\n          options = list(pageLength = 10, dom = 't'))\n\n\n\n\n\n\nKode\n# Hole-level summary\nhole_domain_summary &lt;- hole_domains %&gt;%\n  group_by(hole_domain) %&gt;%\n  summarise(\n    Holes = n(),\n    `Avg Au (ppm)` = mean(avg_au),\n    `Max Au (ppm)` = max(max_au),\n    .groups = 'drop'\n  ) %&gt;%\n  arrange(desc(`Avg Au (ppm)`)) %&gt;%\n  mutate(across(where(is.numeric) & !Holes, ~round(.x, 3)))\n\ndatatable(hole_domain_summary,\n          caption = \"Table 4: Domain Classification by Drillhole\",\n          options = list(pageLength = 10, dom = 't'))\n\n\n\n\n\n\n\n\nDomain Spatial Distribution\n\n\nKode\n# Create color palette for domains\ndomain_colors &lt;- c(\n  \"High Grade Ore\" = \"#d32f2f\",\n  \"Low Grade Ore\" = \"#ff9800\",\n  \"Altered Ore\" = \"#ffd54f\",\n  \"Transitional\" = \"#9e9e9e\",\n  \"Waste\" = \"#e0e0e0\"\n)\n\n# Merge hole domains with collar data\ndomain_spatial &lt;- hole_domains %&gt;%\n  left_join(collar_std, by = \"hole_id\")\n\np_domain_map &lt;- ggplot(domain_spatial, aes(x = x, y = y, color = hole_domain)) +\n  geom_point(size = 4, alpha = 0.8) +\n  scale_color_manual(values = domain_colors) +\n  coord_equal() +\n  labs(\n    title = \"Proposed Domain Distribution - Plan View\",\n    subtitle = \"Dominant domain per drillhole\",\n    x = \"Easting (m)\",\n    y = \"Northing (m)\",\n    color = \"Domain\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\np_domain_map"
  },
  {
    "objectID": "posts/Geological Domain/index.html#the-birth-of-robust-geological-domains",
    "href": "posts/Geological Domain/index.html#the-birth-of-robust-geological-domains",
    "title": "Part 3: Geological Controls & Domain Definition",
    "section": "The Birth of Robust Geological Domains",
    "text": "The Birth of Robust Geological Domains\n\nBefore EDA\nWithout systematic EDA:\n\nRandom data points without geological context\nMixed populations in estimation\nUnreliable variograms\nPoor mining selectivity\n\n\n\nAfter EDA: The 4 Pillars Converge\n\n\n\n\n\nflowchart TB\n    P1[Pillar 1:&lt;br/&gt;Clean, Validated Data] --&gt; D[Robust&lt;br/&gt;Geological&lt;br/&gt;Domains]\n    P2[Pillar 2:&lt;br/&gt;Spatial Understanding] --&gt; D\n    P3[Pillar 3:&lt;br/&gt;Statistical Populations] --&gt; D\n    P4[Pillar 4:&lt;br/&gt;Geological Controls] --&gt; D\n    \n    D --&gt; E1[Reliable Estimation]\n    D --&gt; E2[Realistic Mine Plans]\n    D --&gt; E3[Confident Classification]\n    \n    style D fill:#4caf50,color:#fff\n    style P1 fill:#e3f2fd\n    style P2 fill:#fff3e0\n    style P3 fill:#f3e5f5\n    style P4 fill:#e8f5e9\n\n\n\n\n\n\n\n\nDomain Validation Checklist\n\n\n\n\n\n\nPentingDefensible Domains Must Have:\n\n\n\n✓ Geological justification: Rock types, alteration, structure ✓ Statistical support: Distinct populations, different variability ✓ Spatial continuity: Mappable in 3D space ✓ Grade differences: Economically meaningful ✓ Sample support: Adequate data density ✓ Clear boundaries: Definable contacts"
  },
  {
    "objectID": "posts/Geological Domain/index.html#summary-from-data-to-domains",
    "href": "posts/Geological Domain/index.html#summary-from-data-to-domains",
    "title": "Part 3: Geological Controls & Domain Definition",
    "section": "Summary: From Data to Domains",
    "text": "Summary: From Data to Domains\n\nWhat We Accomplished\nThrough the 4 Pillar EDA framework, we:\n\nValidated data integrity (Pillar 1)\nUnderstood spatial distribution (Pillar 2)\nCharacterized grade populations (Pillar 3)\nIdentified geological controls (Pillar 4)\n\nResult: Scientifically defensible estimation domains ready for resource modeling.\n\n\nNext Steps in Resource Estimation\nWith robust domains defined, you can proceed to:\n\nCompositioning: Regularize sample support within domains\nVariography: Model spatial continuity per domain\nEstimation: Kriging with appropriate search parameters\nClassification: Measured, Indicated, Inferred categories\nValidation: Check estimates against reality\n\n\n\nKey Takeaways\n\n\n\n\n\n\nTipRemember These Principles\n\n\n\n\nEDA is not optional - It’s mandatory for JORC compliance\nData quality = Model reliability - GIGO always applies\nGeology drives domains - Statistics support, geology defines\nDocument everything - Auditors will ask for justification\nUse modern tools - Automate routine tasks, focus on interpretation"
  },
  {
    "objectID": "posts/Geological Domain/index.html#impact-on-business-decisions",
    "href": "posts/Geological Domain/index.html#impact-on-business-decisions",
    "title": "Part 3: Geological Controls & Domain Definition",
    "section": "Impact on Business Decisions",
    "text": "Impact on Business Decisions\nGood EDA and domaining directly impact:\n\nResource confidence: Better classification categories\nMine planning: Realistic extraction sequences\nGrade control: Achievable selectivity\nInvestor confidence: Defensible, auditable reports\nProject value: Reduced risk = higher valuations\n\n\n\n\n\n\n\nPentingThe Foundation of Trust\n\n\n\n“In mining, we don’t get second chances. The quality of our EDA determines whether we build a mine or lose millions in poor decisions.”\nEvery successful operation starts with understanding the data."
  },
  {
    "objectID": "posts/Geological Domain/index.html#tools-and-resources",
    "href": "posts/Geological Domain/index.html#tools-and-resources",
    "title": "Part 3: Geological Controls & Domain Definition",
    "section": "Tools and Resources",
    "text": "Tools and Resources\n\nGeoDataViz Application\nAll analyses demonstrated here are available in GeoDataViz:\n\nGitHub: github.com/ghoziankarami/GeoDataViz\nZenodo: doi.org/10.5281/zenodo.17142676\nLicense: MIT (Open Source)\n\n\n\nContact\n\nAuthor: Ghozian Islam Karami\nEmail: ghoziankarami@gmail.com\nLinkedIn: linkedin.com/in/ghoziankarami"
  },
  {
    "objectID": "posts/Geological Domain/index.html#series-complete",
    "href": "posts/Geological Domain/index.html#series-complete",
    "title": "Part 3: Geological Controls & Domain Definition",
    "section": "Series Complete",
    "text": "Series Complete\nYou’ve completed the comprehensive EDA workflow:\n\n← Part 1: Foundation & Data Validation\n← Part 2: Spatial & Statistical Analysis\nPart 3: Geological Controls & Domain Definition (You are here)\n\nYou now have the tools and knowledge to build trusted geological models from raw drilling data.\n\n“Quality data, systematic analysis, geological thinking - the foundation of every successful mine.”"
  }
]