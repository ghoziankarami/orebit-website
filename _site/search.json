[
  {
    "objectID": "posts/Welcome to Orebit/index.html",
    "href": "posts/Welcome to Orebit/index.html",
    "title": "Welcome to Orbit: Open-Source Geological Analysis Tools",
    "section": "",
    "text": "Orbit Geological Solutions develops open-source R Shiny applications that transform geological data analysis. Our mission: provide enterprise-level analytical tools without expensive licensing fees.\n\n\nTraditional geological software costs $20,000+ annually, creating barriers for: - Small exploration companies - Independent consultants - Geologists in developing regions - Educational institutions\n\n\n\n\n\nComplete geological data analysis platform\n\nInteractive 3D drillhole visualization\nAdvanced statistical analysis & EDA\nComprehensive QA/QC validation\nProfessional reporting tools\n\nTry Demo →\n\n\n\nProfessional geostatistical analysis with automated variogram modeling, kriging, and uncertainty quantification.\n\n\n\nStrategic mine planning with NPV optimization and constraint modeling."
  },
  {
    "objectID": "posts/Welcome to Orebit/index.html#professional-geological-analysis-accessible-to-everyone",
    "href": "posts/Welcome to Orebit/index.html#professional-geological-analysis-accessible-to-everyone",
    "title": "Welcome to Orbit: Open-Source Geological Analysis Tools",
    "section": "",
    "text": "Orbit Geological Solutions develops open-source R Shiny applications that transform geological data analysis. Our mission: provide enterprise-level analytical tools without expensive licensing fees.\n\n\nTraditional geological software costs $20,000+ annually, creating barriers for: - Small exploration companies - Independent consultants - Geologists in developing regions - Educational institutions\n\n\n\n\n\nComplete geological data analysis platform\n\nInteractive 3D drillhole visualization\nAdvanced statistical analysis & EDA\nComprehensive QA/QC validation\nProfessional reporting tools\n\nTry Demo →\n\n\n\nProfessional geostatistical analysis with automated variogram modeling, kriging, and uncertainty quantification.\n\n\n\nStrategic mine planning with NPV optimization and constraint modeling."
  },
  {
    "objectID": "posts/Welcome to Orebit/index.html#why-choose-orbit",
    "href": "posts/Welcome to Orebit/index.html#why-choose-orbit",
    "title": "Welcome to Orbit: Open-Source Geological Analysis Tools",
    "section": "Why Choose Orbit?",
    "text": "Why Choose Orbit?\nCost-Effective: Enterprise capabilities without enterprise costs\nTransparent: Complete visibility into analytical methods\nProfessional: Built by practicing geoscientists for real-world challenges\nAccessible: Cloud-native, runs in any browser"
  },
  {
    "objectID": "posts/Welcome to Orebit/index.html#quick-start-with-r",
    "href": "posts/Welcome to Orebit/index.html#quick-start-with-r",
    "title": "Welcome to Orbit: Open-Source Geological Analysis Tools",
    "section": "Quick Start with R",
    "text": "Quick Start with R\nOrbit applications are built with R. Here’s a simple example of geological data visualization:\n\nlibrary(tidyverse)\nlibrary(plotly)\n\n# Sample drillhole data\ncollar_data &lt;- tibble(\n  HoleID = paste0(\"DH\", 1:20),\n  X = runif(20, 100, 500),\n  Y = runif(20, 100, 500),\n  Z = runif(20, 50, 200),\n  Grade = rlnorm(20, 0, 0.5)\n)\n\n# Interactive 3D visualization\nplot_ly(collar_data, x = ~X, y = ~Y, z = ~Z, \n        color = ~Grade, type = 'scatter3d', mode = 'markers',\n        marker = list(size = 8, colorscale = 'Viridis'),\n        text = ~paste(\"Hole:\", HoleID, \"&lt;br&gt;Grade:\", round(Grade, 2))) %&gt;%\n  layout(title = \"Drillhole Collar Map\",\n         scene = list(\n           xaxis = list(title = \"Easting\"),\n           yaxis = list(title = \"Northing\"),\n           zaxis = list(title = \"Elevation\")\n         ))"
  },
  {
    "objectID": "posts/Welcome to Orebit/index.html#learning-resources",
    "href": "posts/Welcome to Orebit/index.html#learning-resources",
    "title": "Welcome to Orbit: Open-Source Geological Analysis Tools",
    "section": "Learning Resources",
    "text": "Learning Resources\n\nTechnical Blog\nComprehensive guides on: - Exploratory data analysis workflows - Data preprocessing techniques - Geostatistical modeling - Resource estimation best practices\n\n\nCommunity\n\nGitHub: github.com/orbit-geo\nLinkedIn: Professional networking and updates\nInstagram: @orebit.id - Visual tutorials"
  },
  {
    "objectID": "posts/Welcome to Orebit/index.html#get-started",
    "href": "posts/Welcome to Orebit/index.html#get-started",
    "title": "Welcome to Orbit: Open-Source Geological Analysis Tools",
    "section": "Get Started",
    "text": "Get Started\nFor Individuals: Try GeoDataViz demo and explore tutorials\nFor Companies: Contact us for enterprise deployment\nFor Developers: Contribute to open-source development\nLaunch GeoDataViz View Documentation\n\nOrbit Geological Solutions - Making professional geological analysis accessible to everyone."
  },
  {
    "objectID": "posts/Geological Domain/index.html",
    "href": "posts/Geological Domain/index.html",
    "title": "Part 3: Geological Controls & Domain Definition",
    "section": "",
    "text": "We’ve validated our data (Part 1), explored spatial patterns, and characterized statistical distributions (Part 2). Now comes the most critical step: connecting numbers to geology.\nPillar 4: Geological Controls Analysis transforms statistical insights into geologically defensible estimation domains - the backbone of reliable resource models."
  },
  {
    "objectID": "posts/Geological Domain/index.html#introduction",
    "href": "posts/Geological Domain/index.html#introduction",
    "title": "Part 3: Geological Controls & Domain Definition",
    "section": "",
    "text": "We’ve validated our data (Part 1), explored spatial patterns, and characterized statistical distributions (Part 2). Now comes the most critical step: connecting numbers to geology.\nPillar 4: Geological Controls Analysis transforms statistical insights into geologically defensible estimation domains - the backbone of reliable resource models."
  },
  {
    "objectID": "posts/Geological Domain/index.html#the-critical-question",
    "href": "posts/Geological Domain/index.html#the-critical-question",
    "title": "Part 3: Geological Controls & Domain Definition",
    "section": "The Critical Question",
    "text": "The Critical Question\n\n“Where is mineralization located and WHY?”\n\nThis isn’t just about mapping high grades. It’s about understanding the geological controls that govern mineralization:\n\nWhich rock types host mineralization?\nAre there geochemical relationships between elements?\nHow does grade behave at geological contacts?\nWhat structural controls exist?\n\n\n\n\n\n\n\nPentingWhy Geological Domains Matter\n\n\n\nPoor domaining is a leading cause of resource estimation failure:\n\nMixed populations create unrealistic variograms\nInappropriate kriging produces smoothing artifacts\nClassification confidence is compromised\nMining selectively targets become unrealistic\n\nGood domains = Reliable estimates"
  },
  {
    "objectID": "posts/Geological Domain/index.html#setup",
    "href": "posts/Geological Domain/index.html#setup",
    "title": "Part 3: Geological Controls & Domain Definition",
    "section": "Setup",
    "text": "Setup\n\n\nKode\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(DT)\nlibrary(RColorBrewer)\nlibrary(GGally)\nlibrary(patchwork)\n\n# Create simulated drilling data with geological controls\nset.seed(456)\nn_holes &lt;- 50\n\ncollar &lt;- data.frame(\n  hole_id = paste0(\"DDH\", sprintf(\"%03d\", 1:n_holes)),\n  x = runif(n_holes, 500000, 501000),\n  y = runif(n_holes, 9000000, 9001000),\n  rl = runif(n_holes, 100, 200)\n)\n\n# Generate assay data with lithology-controlled grades\nlitho_codes &lt;- c(\"Andesite\", \"Diorite\", \"Mineralized_Zone\", \"Altered_Volcanics\")\nlitho_grade_means &lt;- c(0.5, 0.8, 3.5, 2.0)  # Different grades by lithology\n\nassay_list &lt;- lapply(1:n_holes, function(i) {\n  n_intervals &lt;- sample(15:25, 1)\n  depths &lt;- seq(0, by = 2, length.out = n_intervals)\n  \n  # Assign lithology to each interval\n  litho_idx &lt;- sample(1:length(litho_codes), n_intervals - 1, replace = TRUE,\n                     prob = c(0.3, 0.2, 0.3, 0.2))\n  \n  # Generate grades based on lithology\n  au_grades &lt;- sapply(litho_idx, function(idx) {\n    max(0, rnorm(1, mean = litho_grade_means[idx], sd = 1.5))\n  })\n  \n  # Correlated Ag and Cu\n  ag_grades &lt;- au_grades * runif(n_intervals - 1, 8, 12) + rnorm(n_intervals - 1, 0, 5)\n  cu_grades &lt;- au_grades * 0.3 + rnorm(n_intervals - 1, 0, 0.3)\n  \n  data.frame(\n    hole_id = collar$hole_id[i],\n    from = depths[-length(depths)],\n    to = depths[-1],\n    au_ppm = pmax(0, au_grades),\n    ag_ppm = pmax(0, ag_grades),\n    cu_pct = pmax(0, cu_grades),\n    lithology = litho_codes[litho_idx]\n  )\n})\n\ncombined_data &lt;- do.call(rbind, assay_list) %&gt;%\n  left_join(collar, by = \"hole_id\") %&gt;%\n  janitor::clean_names()\n\n# Prepare standard formats\ncollar_std &lt;- collar %&gt;% janitor::clean_names()\nassay_std &lt;- combined_data %&gt;% select(hole_id, from, to, au_ppm, ag_ppm, cu_pct)\nlithology_std &lt;- combined_data %&gt;% select(hole_id, from, to, lithology)"
  },
  {
    "objectID": "posts/Geological Domain/index.html#part-a-lithology-analysis",
    "href": "posts/Geological Domain/index.html#part-a-lithology-analysis",
    "title": "Part 3: Geological Controls & Domain Definition",
    "section": "Part A: Lithology Analysis",
    "text": "Part A: Lithology Analysis\n\nGrade Distribution by Rock Type\nThe most fundamental control: which rocks contain ore?\n\n\nKode\n# Calculate statistics by lithology\nlitho_summary &lt;- combined_data %&gt;%\n  group_by(lithology) %&gt;%\n  summarise(\n    n = n(),\n    mean_au = mean(au_ppm, na.rm = TRUE),\n    median_au = median(au_ppm, na.rm = TRUE),\n    max_au = max(au_ppm, na.rm = TRUE),\n    sd_au = sd(au_ppm, na.rm = TRUE)\n  ) %&gt;%\n  arrange(desc(mean_au))\n\n# Create color palette\nn_litho &lt;- length(unique(combined_data$lithology))\nlitho_colors &lt;- setNames(\n  brewer.pal(min(n_litho, 9), \"Set1\"),\n  unique(combined_data$lithology)\n)\n\n# Boxplot with statistical annotations\np_litho_box &lt;- ggplot(combined_data, aes(x = reorder(lithology, au_ppm, FUN = median), \n                                          y = au_ppm, \n                                          fill = lithology)) +\n  geom_boxplot(outlier.alpha = 0.3) +\n  stat_summary(fun = mean, geom = \"point\", shape = 23, size = 3, \n               fill = \"red\", color = \"darkred\") +\n  scale_fill_manual(values = litho_colors) +\n  labs(\n    title = \"Gold Grade Distribution by Lithology\",\n    subtitle = \"Box = IQR, Diamond = Mean, Line = Median\",\n    x = \"Lithology (ordered by median grade)\",\n    y = \"Au (ppm)\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.position = \"none\"\n  )\n\np_litho_box\n\n\n\n\n\n\n\n\n\n\n\nStatistical Summary by Lithology\n\n\nKode\ndatatable(litho_summary,\n          caption = \"Table 1: Gold Statistics by Lithology\",\n          options = list(pageLength = 10)) %&gt;%\n  formatRound(columns = c('mean_au', 'median_au', 'max_au', 'sd_au'), digits = 3) %&gt;%\n  formatStyle(\n    'mean_au',\n    background = styleColorBar(litho_summary$mean_au, 'lightblue'),\n    backgroundSize = '100% 90%',\n    backgroundRepeat = 'no-repeat',\n    backgroundPosition = 'center'\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nCatatanGeological Interpretation\n\n\n\nFrom this analysis, identify:\n\nPrimary ore hosts: Lithologies with highest mean/median grades\nBarren waste: Low-grade lithologies to exclude\nGrade variance: High CV suggests mixed mineralization styles\nOutlier behavior: Which rocks have extreme values?\n\nAction: Candidate lithologies for separate estimation domains\n\n\n\n\nGrade-Tonnage Curves by Lithology\nUnderstanding the economic potential of each rock type.\n\n\nKode\n# Calculate grade-tonnage curves\ncutoffs &lt;- seq(0, 5, by = 0.1)\n\ngt_curves &lt;- combined_data %&gt;%\n  expand_grid(cutoff = cutoffs) %&gt;%\n  filter(au_ppm &gt;= cutoff) %&gt;%\n  group_by(lithology, cutoff) %&gt;%\n  summarise(\n    tonnage_pct = n() / nrow(combined_data) * 100,\n    avg_grade = mean(au_ppm, na.rm = TRUE),\n    .groups = 'drop'\n  )\n\n# Plot tonnage vs cutoff\np_tonnage &lt;- ggplot(gt_curves, aes(x = cutoff, y = tonnage_pct, color = lithology)) +\n  geom_line(size = 1.2) +\n  scale_color_manual(values = litho_colors) +\n  labs(\n    title = \"Tonnage vs Cut-off Grade\",\n    x = \"Cut-off Grade (ppm Au)\",\n    y = \"Tonnage (% of total)\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nKode\n# Plot grade vs cutoff\np_grade &lt;- ggplot(gt_curves, aes(x = cutoff, y = avg_grade, color = lithology)) +\n  geom_line(size = 1.2) +\n  scale_color_manual(values = litho_colors) +\n  labs(\n    title = \"Average Grade vs Cut-off\",\n    x = \"Cut-off Grade (ppm Au)\",\n    y = \"Average Grade (ppm)\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\np_tonnage / p_grade"
  },
  {
    "objectID": "posts/Geological Domain/index.html#part-b-element-correlation-analysis",
    "href": "posts/Geological Domain/index.html#part-b-element-correlation-analysis",
    "title": "Part 3: Geological Controls & Domain Definition",
    "section": "Part B: Element Correlation Analysis",
    "text": "Part B: Element Correlation Analysis\n\nBivariate Relationships\nUnderstanding element associations reveals mineralization processes.\n\n\nKode\n# Select grade variables for correlation\ngrade_vars &lt;- c(\"au_ppm\", \"ag_ppm\", \"cu_pct\")\n\n# Create scatter plot matrix\nggpairs(\n  combined_data %&gt;% select(all_of(grade_vars), lithology),\n  mapping = aes(color = lithology, alpha = 0.5),\n  upper = list(continuous = wrap(\"cor\", size = 3)),\n  lower = list(continuous = wrap(\"points\", alpha = 0.3, size = 0.5)),\n  diag = list(continuous = wrap(\"densityDiag\", alpha = 0.5))\n) +\n  scale_color_manual(values = litho_colors) +\n  scale_fill_manual(values = litho_colors) +\n  theme_minimal() +\n  labs(title = \"Element Correlation Matrix by Lithology\")\n\n\nWarning: No shared levels found between `names(values)` of the manual scale and the\ndata's colour values.\nNo shared levels found between `names(values)` of the manual scale and the\ndata's colour values.\n\n\nWarning: No shared levels found between `names(values)` of the manual scale and the\ndata's fill values.\n\n\nWarning: No shared levels found between `names(values)` of the manual scale and the\ndata's colour values.\nNo shared levels found between `names(values)` of the manual scale and the\ndata's colour values.\n\n\nWarning: No shared levels found between `names(values)` of the manual scale and the\ndata's fill values.\n\n\nWarning: No shared levels found between `names(values)` of the manual scale and the\ndata's colour values.\nNo shared levels found between `names(values)` of the manual scale and the\ndata's colour values.\n\n\nWarning: No shared levels found between `names(values)` of the manual scale and the\ndata's fill values.\n\n\nWarning: No shared levels found between `names(values)` of the manual scale and the\ndata's colour values.\nNo shared levels found between `names(values)` of the manual scale and the\ndata's colour values.\n\n\nWarning: No shared levels found between `names(values)` of the manual scale and the\ndata's fill values.\n\n\nWarning: No shared levels found between `names(values)` of the manual scale and the\ndata's colour values.\nNo shared levels found between `names(values)` of the manual scale and the\ndata's colour values.\n\n\nWarning: No shared levels found between `names(values)` of the manual scale and the\ndata's fill values.\nNo shared levels found between `names(values)` of the manual scale and the\ndata's fill values.\n\n\nWarning: No shared levels found between `names(values)` of the manual scale and the\ndata's colour values.\nNo shared levels found between `names(values)` of the manual scale and the\ndata's colour values.\n\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\nWarning: No shared levels found between `names(values)` of the manual scale and the\ndata's colour values.\n\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\nWarning: No shared levels found between `names(values)` of the manual scale and the\ndata's colour values.\n\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\nWarning: No shared levels found between `names(values)` of the manual scale and the\ndata's colour values.\nNo shared levels found between `names(values)` of the manual scale and the\ndata's colour values.\n\n\n\n\n\n\n\n\n\n\n\nRegression Analysis: Au vs Ag\n\n\nKode\n# Calculate regression by lithology\nregression_data &lt;- combined_data %&gt;%\n  filter(!is.na(au_ppm) & !is.na(ag_ppm)) %&gt;%\n  group_by(lithology) %&gt;%\n  filter(n() &gt; 10) %&gt;%\n  nest() %&gt;%\n  mutate(\n    model = purrr::map(data, ~lm(ag_ppm ~ au_ppm, data = .x)),\n    r_squared = purrr::map_dbl(model, ~summary(.x)$r.squared),\n    slope = purrr::map_dbl(model, ~coef(.x)[2]),\n    intercept = purrr::map_dbl(model, ~coef(.x)[1])\n  )\n\n# Create scatter plot with regression lines\np_regression &lt;- ggplot(combined_data, aes(x = au_ppm, y = ag_ppm)) +\n  geom_point(aes(color = lithology), alpha = 0.5, size = 2) +\n  geom_smooth(aes(color = lithology), method = \"lm\", se = FALSE, size = 1.2) +\n  scale_color_manual(values = litho_colors) +\n  labs(\n    title = \"Gold vs Silver: Correlation by Lithology\",\n    subtitle = \"Different slopes suggest different mineralization styles\",\n    x = \"Au (ppm)\",\n    y = \"Ag (ppm)\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\np_regression\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nCorrelation Summary Table\n\n\nKode\nregression_summary &lt;- regression_data %&gt;%\n  select(lithology, r_squared, slope, intercept) %&gt;%\n  arrange(desc(r_squared)) %&gt;%\n  mutate(\n    equation = paste0(\"Ag = \", round(slope, 2), \" × Au + \", round(intercept, 2)),\n    r_squared = round(r_squared, 3)\n  ) %&gt;%\n  select(lithology, r_squared, equation)\n\ndatatable(regression_summary,\n          caption = \"Table 2: Au-Ag Correlation by Lithology\",\n          options = list(pageLength = 10, dom = 't')) %&gt;%\n  formatStyle(\n    'r_squared',\n    background = styleColorBar(regression_summary$r_squared, 'lightgreen'),\n    backgroundSize = '100% 90%',\n    backgroundRepeat = 'no-repeat',\n    backgroundPosition = 'center'\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nTipGeochemical Insights\n\n\n\nStrong correlations indicate:\n\nCoupled mineralization: Elements deposited together\nCommon sources: Shared ore fluids or processes\nDomain boundaries: Changes in correlation suggest domain breaks\n\nWeak correlations may indicate:\n\nDifferent mineralization events: Overprinting\nRemobilization: Secondary processes\nMixed populations: Need for further domain subdivision"
  },
  {
    "objectID": "posts/Geological Domain/index.html#part-c-contact-analysis",
    "href": "posts/Geological Domain/index.html#part-c-contact-analysis",
    "title": "Part 3: Geological Controls & Domain Definition",
    "section": "Part C: Contact Analysis",
    "text": "Part C: Contact Analysis\n\nGrade Behavior at Geological Boundaries\nHow does grade change across lithology contacts?\n#| fig-width: 10 #| fig-height: 6"
  },
  {
    "objectID": "posts/Drillhole Data Preprocessing/index.html",
    "href": "posts/Drillhole Data Preprocessing/index.html",
    "title": "Pre-Processing Drillhole Data for Resource Estimation",
    "section": "",
    "text": "Poor preprocessing = Poor resource estimates\nPreprocessing failures account for most resource estimation errors: - 30% grade overestimation from wrong composite lengths - Domain boundary failures creating artificial continuity - Outlier treatment errors causing bias"
  },
  {
    "objectID": "posts/Drillhole Data Preprocessing/index.html#why-preprocessing-matters",
    "href": "posts/Drillhole Data Preprocessing/index.html#why-preprocessing-matters",
    "title": "Pre-Processing Drillhole Data for Resource Estimation",
    "section": "",
    "text": "Poor preprocessing = Poor resource estimates\nPreprocessing failures account for most resource estimation errors: - 30% grade overestimation from wrong composite lengths - Domain boundary failures creating artificial continuity - Outlier treatment errors causing bias"
  },
  {
    "objectID": "posts/Drillhole Data Preprocessing/index.html#the-4-step-preprocessing-workflow",
    "href": "posts/Drillhole Data Preprocessing/index.html#the-4-step-preprocessing-workflow",
    "title": "Pre-Processing Drillhole Data for Resource Estimation",
    "section": "The 4-Step Preprocessing Workflow",
    "text": "The 4-Step Preprocessing Workflow\n\nStep 1: Desurveying\nTransform survey data into 3D coordinates:\n\nlibrary(tidyverse)\nlibrary(plotly)\n\n# Sample data\ncollar &lt;- tibble(\n  HoleID = \"DH001\",\n  X = 1000, Y = 2000, Z = 500\n)\n\nsurvey &lt;- tibble(\n  HoleID = \"DH001\",\n  Depth = c(0, 50, 100, 150),\n  Azimuth = c(0, 0, 5, 10),\n  Dip = c(-90, -85, -80, -75)\n)\n\nassay &lt;- tibble(\n  HoleID = \"DH001\",\n  From = seq(0, 145, 5),\n  To = seq(5, 150, 5),\n  Grade = rlnorm(30, meanlog = 0.3, sdlog = 0.6)\n)\n\n# Desurveying function\ndesurvey_hole &lt;- function(collar, survey, assay) {\n  \n  # Interpolate survey at sample midpoints\n  assay_3d &lt;- assay %&gt;%\n    mutate(\n      Midpoint = (From + To) / 2,\n      # Find survey interval\n      Survey_Idx = findInterval(Midpoint, survey$Depth),\n      # Get survey values\n      Azimuth = survey$Azimuth[pmin(Survey_Idx + 1, nrow(survey))],\n      Dip = survey$Dip[pmin(Survey_Idx + 1, nrow(survey))],\n      # Convert to radians\n      Az_rad = Azimuth * pi / 180,\n      Dip_rad = Dip * pi / 180,\n      # Calculate displacement\n      Length = To - From,\n      DX = Length * sin(Az_rad) * cos(Dip_rad),\n      DY = Length * cos(Az_rad) * cos(Dip_rad),\n      DZ = Length * sin(Dip_rad)\n    ) %&gt;%\n    mutate(\n      # Calculate cumulative coordinates\n      X = collar$X + cumsum(DX),\n      Y = collar$Y + cumsum(DY),\n      Z = collar$Z + cumsum(DZ)\n    )\n  \n  return(assay_3d)\n}\n\n# Apply desurveying\ndesurveyed &lt;- desurvey_hole(collar, survey, assay)\n\n# Visualize\nplot_ly(desurveyed, x = ~X, y = ~Y, z = ~Z,\n        color = ~Grade, type = 'scatter3d', mode = 'markers',\n        marker = list(size = 5, colorscale = 'Viridis', \n                     colorbar = list(title = \"Grade\"))) %&gt;%\n  layout(title = \"Desurveyed Drillhole\",\n         scene = list(\n           xaxis = list(title = \"X\"),\n           yaxis = list(title = \"Y\"),\n           zaxis = list(title = \"Z\", autorange = \"reversed\")\n         ))\n\n\n\n\n\n\n\nStep 2: Compositing\nRegularize sample lengths:\n\n# Compositing function (5m length)\ncomposite_data &lt;- function(data, comp_length = 5) {\n  \n  composites &lt;- data %&gt;%\n    mutate(\n      # Assign composite interval\n      Comp_ID = floor(From / comp_length)\n    ) %&gt;%\n    group_by(Comp_ID) %&gt;%\n    summarise(\n      From = min(From),\n      To = max(To),\n      Length = To - From,\n      Grade = weighted.mean(Grade, To - From),\n      X = mean(X),\n      Y = mean(Y),\n      Z = mean(Z),\n      N_Samples = n(),\n      .groups = 'drop'\n    ) %&gt;%\n    filter(Length &gt;= comp_length * 0.5)  # Keep composites &gt; 50% target length\n  \n  return(composites)\n}\n\n# Create composites\ncomposites &lt;- composite_data(desurveyed, comp_length = 5)\n\ncat(\"Original samples:\", nrow(desurveyed), \"\\n\")\n\nOriginal samples: 30 \n\ncat(\"Composites:\", nrow(composites), \"\\n\")\n\nComposites: 30 \n\ncat(\"Average composite length:\", round(mean(composites$Length), 2), \"m\\n\")\n\nAverage composite length: 5 m\n\n# Display\nhead(composites %&gt;% select(Comp_ID, From, To, Length, Grade, X, Y, Z))\n\n# A tibble: 6 × 8\n  Comp_ID  From    To Length Grade     X     Y     Z\n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1       0     0     5      5 0.882  1000 2000.  495.\n2       1     5    10      5 3.16   1000 2001.  490.\n3       2    10    15      5 1.59   1000 2001.  485.\n4       3    15    20      5 0.729  1000 2002.  480.\n5       4    20    25      5 0.850  1000 2002.  475.\n6       5    25    30      5 0.856  1000 2003.  470.\n\n\nComposite Length Selection: - Too short: Excessive smoothing, lose geological detail - Too long: Sample non-stationarity, poor statistics - Typical: 1-5m for most deposits\n\n\nStep 3: Outlier Treatment\nIdentify and handle extreme values:\n\n# Calculate percentile thresholds\nP95 &lt;- quantile(composites$Grade, 0.95)\nP99 &lt;- quantile(composites$Grade, 0.99)\n\n# Identify outliers\noutlier_summary &lt;- composites %&gt;%\n  summarise(\n    Mean = mean(Grade),\n    Median = median(Grade),\n    P95 = P95,\n    P99 = P99,\n    N_Above_P95 = sum(Grade &gt; P95),\n    N_Above_P99 = sum(Grade &gt; P99)\n  )\n\nprint(outlier_summary)\n\n# A tibble: 1 × 6\n   Mean Median   P95   P99 N_Above_P95 N_Above_P99\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;       &lt;int&gt;       &lt;int&gt;\n1  1.46   1.31  2.86  3.54           2           1\n\n# Top-cutting demonstration\ncomposites_capped &lt;- composites %&gt;%\n  mutate(\n    Grade_Original = Grade,\n    Grade_Capped = pmin(Grade, P95)  # Cap at 95th percentile\n  )\n\n# Comparison plot\np1 &lt;- plot_ly(composites, x = ~Grade, type = \"histogram\", \n              name = \"Original\", nbinsx = 20) %&gt;%\n  layout(xaxis = list(title = \"Grade\"))\n\np2 &lt;- plot_ly(composites_capped, x = ~Grade_Capped, type = \"histogram\",\n              name = \"Capped (P95)\", nbinsx = 20) %&gt;%\n  layout(xaxis = list(title = \"Grade\"))\n\nsubplot(p1, p2, nrows = 2, shareX = TRUE) %&gt;%\n  layout(title = \"Effect of Grade Capping\")\n\n\n\nOutlier analysis and capping\n\n\nCapping Strategies: - Statistical: Percentile-based (95th, 98th) - Geological: Based on domain/mineralization type - Spatial: Consider outlier distribution\n\n\nStep 4: Domain Assignment\nAssign samples to geological domains:\n\n# Simple domain assignment (by elevation)\ncomposites_domains &lt;- composites %&gt;%\n  mutate(\n    Domain = case_when(\n      Z &gt; 480 ~ \"Oxide\",\n      Z &gt; 460 ~ \"Transition\",\n      TRUE ~ \"Fresh\"\n    )\n  )\n\n# Domain statistics\ndomain_stats &lt;- composites_domains %&gt;%\n  group_by(Domain) %&gt;%\n  summarise(\n    N_Samples = n(),\n    Mean_Grade = mean(Grade),\n    SD_Grade = sd(Grade),\n    Min_Grade = min(Grade),\n    Max_Grade = max(Grade),\n    .groups = 'drop'\n  )\n\nprint(domain_stats)\n\n# A tibble: 3 × 6\n  Domain     N_Samples Mean_Grade SD_Grade Min_Grade Max_Grade\n  &lt;chr&gt;          &lt;int&gt;      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 Fresh             22       1.46    0.764     0.455      3.70\n2 Oxide              4       1.59    1.11      0.729      3.16\n3 Transition         4       1.30    0.514     0.850      1.77\n\n# Visualize domains\nplot_ly(composites_domains, x = ~X, y = ~Y, z = ~Z,\n        color = ~Domain, colors = c(\"orange\", \"yellow\", \"green\"),\n        type = 'scatter3d', mode = 'markers',\n        marker = list(size = 5)) %&gt;%\n  layout(title = \"Geological Domains\",\n         scene = list(\n           xaxis = list(title = \"X\"),\n           yaxis = list(title = \"Y\"),\n           zaxis = list(title = \"Z\", autorange = \"reversed\")\n         ))"
  },
  {
    "objectID": "posts/Drillhole Data Preprocessing/index.html#quality-checks-before-estimation",
    "href": "posts/Drillhole Data Preprocessing/index.html#quality-checks-before-estimation",
    "title": "Pre-Processing Drillhole Data for Resource Estimation",
    "section": "Quality Checks Before Estimation",
    "text": "Quality Checks Before Estimation\n\nChecklist:\n\n# Automated QA checks\nqa_results &lt;- list(\n  \"Total composites\" = nrow(composites),\n  \"Composite length CV\" = round(sd(composites$Length)/mean(composites$Length), 2),\n  \"Grade CV\" = round(sd(composites$Grade)/mean(composites$Grade), 2),\n  \"Domains defined\" = n_distinct(composites_domains$Domain),\n  \"Outliers (&gt;P95)\" = sum(composites$Grade &gt; P95),\n  \"Missing coords\" = sum(is.na(composites$X) | is.na(composites$Y) | is.na(composites$Z))\n)\n\ncat(\"QA/QC Summary:\\n\")\n\nQA/QC Summary:\n\nfor(i in seq_along(qa_results)) {\n  cat(sprintf(\"- %s: %s\\n\", names(qa_results)[i], qa_results[[i]]))\n}\n\n- Total composites: 30\n- Composite length CV: 0\n- Grade CV: 0.52\n- Domains defined: 3\n- Outliers (&gt;P95): 2\n- Missing coords: 0\n\n\nPass Criteria: - [ ] Composite length CV &lt; 0.3 - [ ] All coordinates valid - [ ] Outliers documented and justified - [ ] Domains statistically different"
  },
  {
    "objectID": "posts/Drillhole Data Preprocessing/index.html#using-geodataviz-for-preprocessing",
    "href": "posts/Drillhole Data Preprocessing/index.html#using-geodataviz-for-preprocessing",
    "title": "Pre-Processing Drillhole Data for Resource Estimation",
    "section": "Using GeoDataViz for Preprocessing",
    "text": "Using GeoDataViz for Preprocessing\nGeoDataViz automates this entire workflow:\nFeatures: 1. Auto-desurveying: Multiple survey interpolation methods 2. Smart compositing: Geological boundary preservation 3. Outlier detection: Multiple statistical methods 4. Domain tools: Interactive 3D domain definition 5. Export: CSV, Excel, or database formats\nTry GeoDataViz →"
  },
  {
    "objectID": "posts/Drillhole Data Preprocessing/index.html#next-steps",
    "href": "posts/Drillhole Data Preprocessing/index.html#next-steps",
    "title": "Pre-Processing Drillhole Data for Resource Estimation",
    "section": "Next Steps",
    "text": "Next Steps\nAfter preprocessing: 1. Variogram Modeling → Spatial continuity analysis 2. Block Modeling → Create estimation grid 3. Kriging → Resource estimation 4. Validation → Model quality checks\nNext Guide: Coming soon - “Variogram Modeling with GeoDataViz”"
  },
  {
    "objectID": "posts/Drillhole Data Preprocessing/index.html#summary-code",
    "href": "posts/Drillhole Data Preprocessing/index.html#summary-code",
    "title": "Pre-Processing Drillhole Data for Resource Estimation",
    "section": "Summary Code",
    "text": "Summary Code\nComplete preprocessing pipeline:\n\n# Complete workflow\npreprocessing_pipeline &lt;- function(collar, survey, assay, comp_length = 5) {\n  \n  # 1. Desurvey\n  desurveyed &lt;- desurvey_hole(collar, survey, assay)\n  \n  # 2. Composite\n  composites &lt;- composite_data(desurveyed, comp_length)\n  \n  # 3. Cap outliers\n  P95 &lt;- quantile(composites$Grade, 0.95)\n  composites$Grade_Capped &lt;- pmin(composites$Grade, P95)\n  \n  # 4. Assign domains\n  composites$Domain &lt;- case_when(\n    composites$Z &gt; 480 ~ \"Oxide\",\n    composites$Z &gt; 460 ~ \"Transition\",\n    TRUE ~ \"Fresh\"\n  )\n  \n  return(composites)\n}\n\n\nFor step-by-step video tutorials, visit @orebit.id"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Digitizing Geology, From Exploration to Production",
    "section": "",
    "text": "Professional R Shiny applications for modern geology. We provide an advanced geological analysis suite—including GeoDataViz, Resource Estimation, and Pit Optimization tools—for professional geoscientists and mining engineers. Built by geologists, for geologists.\n\n Launch GeoDataViz Demo  Read Technical Guides\n\n\n\n\nAccess enterprise-level tools without the enterprise-level costs. We bridge the gap between powerful analytics and accessibility.\n\n\n\n\n\n\n\nEliminate prohibitive $20,000+ annual licensing fees. Powerful tools should be accessible to all, not just the largest corporations.\n\n\n\n\n\n\n\n\nDeveloped by a Certified Competent Person (CP) who understands the real-world challenges and workflows of the mining industry.\n\n\n\n\n\n\n\n\nBuilt on open-source R technology. No black-box algorithms. Our methods are clear, ensuring reproducibility and scientific integrity.\n\n\n\n\n\n\n\n\nNo installation, no maintenance. Run sophisticated analyses directly in your browser, anywhere, on any device.\n\n\n\n\n\n\n\n\n\n\nAvailable Now\n\nTransform your geological workflow with professional-grade exploratory data analysis and 3D visualization, built specifically for drillhole datasets. Launch Free Demo →\n\n\n\n\nComing Q2 2026\n\nProfessional geostatistical analysis and resource modeling. Advanced kriging, variogram modeling, and uncertainty quantification. Join Beta List →\n\n\n\n\nComing Q3 2026\n\nSophisticated mine planning and optimization algorithms. Maximize NPV while considering operational constraints and environmental factors. Get Notified →\n\n\n\n\n\n\nStay updated on our upcoming resource estimation and pit optimization tools.\n\n Get Notified"
  },
  {
    "objectID": "index.html#why-orebit-solutions",
    "href": "index.html#why-orebit-solutions",
    "title": "Digitizing Geology, From Exploration to Production",
    "section": "",
    "text": "Access enterprise-level tools without the enterprise-level costs. We bridge the gap between powerful analytics and accessibility.\n\n\n\n\n\n\n\nEliminate prohibitive $20,000+ annual licensing fees. Powerful tools should be accessible to all, not just the largest corporations.\n\n\n\n\n\n\n\n\nDeveloped by a Certified Competent Person (CP) who understands the real-world challenges and workflows of the mining industry.\n\n\n\n\n\n\n\n\nBuilt on open-source R technology. No black-box algorithms. Our methods are clear, ensuring reproducibility and scientific integrity.\n\n\n\n\n\n\n\n\nNo installation, no maintenance. Run sophisticated analyses directly in your browser, anywhere, on any device."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Orebit",
    "section": "",
    "text": "Orebit Geological Solutions develops professional R Shiny applications that transform how geoscientists and mining engineers approach data analysis. Our mission is to provide cost-effective, transparent, and powerful analytical tools that rival expensive proprietary software—without the prohibitive licensing fees."
  },
  {
    "objectID": "about.html#advanced-geological-solutions-for-the-modern-mining-industry",
    "href": "about.html#advanced-geological-solutions-for-the-modern-mining-industry",
    "title": "About Orebit",
    "section": "",
    "text": "Orebit Geological Solutions develops professional R Shiny applications that transform how geoscientists and mining engineers approach data analysis. Our mission is to provide cost-effective, transparent, and powerful analytical tools that rival expensive proprietary software—without the prohibitive licensing fees."
  },
  {
    "objectID": "about.html#the-challenge-we-address",
    "href": "about.html#the-challenge-we-address",
    "title": "About Orebit",
    "section": "The Challenge We Address",
    "text": "The Challenge We Address\nThe mining industry faces a critical paradox: geological analysis demands increasingly sophisticated tools, yet traditional software solutions create barriers through:\n\nHigh Costs: Enterprise geological software can exceed $50,000 per year in licensing fees\nVendor Lock-in: Proprietary systems limit flexibility and customization\nLimited Transparency: Black-box algorithms make validation and reproducibility difficult\nAccessibility Gaps: Small companies and consultants struggle to afford industry-standard tools\n\nOrebit exists to eliminate these barriers."
  },
  {
    "objectID": "about.html#our-product-philosophy",
    "href": "about.html#our-product-philosophy",
    "title": "About Orebit",
    "section": "Our Product Philosophy",
    "text": "Our Product Philosophy\n\nOpen-Source Foundation\nAll Orebit applications are built on open-source technologies, ensuring: - Complete transparency in analytical methods - Full reproducibility of results - No vendor lock-in or hidden dependencies - Community-driven development and improvement\n\n\nUniversal Accessibility\nCloud-native architecture means: - No installation required - Run directly in web browsers - Access from anywhere - Cross-platform compatibility"
  },
  {
    "objectID": "about.html#the-team-behind-orebit",
    "href": "about.html#the-team-behind-orebit",
    "title": "About Orebit",
    "section": "The Team Behind Orebit",
    "text": "The Team Behind Orebit\nOrebit is led by Ghozian Islam Karami, Senior Geologist and Certified BNSP Competent Person (CP) with over a decade of mining industry experience. His expertise in offshore tin exploration, resource evaluation, and geological data analysis ensures that every Orebit application addresses real-world geological challenges.\nCore Expertise: - Geostatistics and resource modeling - R programming and Shiny application development - Quality assurance systems for mining operations - Geospatial analysis and 3D geological modeling\nProfessional Credentials: - Certified Competent Person (CP) - BNSP - M.Sc. Geological Sciences - KFUPM - B.Sc. Geology - Universitas Padjadjaran - Senior Geologist - PERHAPI\nThis combination of geological expertise and technical development skills ensures Orebit applications are both geologically sound and technically sophisticated."
  },
  {
    "objectID": "about.html#contact-information",
    "href": "about.html#contact-information",
    "title": "About Orebit",
    "section": "Contact Information",
    "text": "Contact Information\nEmail: ghoziankarami@gmail.com\nGitHub: github.com/ghoziankarami\nLinkedIn: Ghozian Islam Karami"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Technical Resources",
    "section": "",
    "text": "Welcome to the Orebit Knowledge Hub\n\n\nHere you’ll find in-depth articles, practical tutorials, and case studies designed to help you master modern geological data analysis. Whether you’re using our flagship application, GeoDataViz, or exploring advanced open-source workflows in R, these resources are here to support your work.\n\n\n\n\n\n\n\n\n   \n    \n    \n      Urut berdasar\n      Default\n      \n        Judul\n      \n      \n        Tanggal - Paling lama\n      \n      \n        Tanggal - Paling baru\n      \n      \n        Penulis\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nPre-Processing Drillhole Data for Resource Estimation\n\n\n\nGeoDataViz\n\nPreprocessing\n\nResource-Estimation\n\n\n\nComplete workflow for transforming raw drillhole data into estimation-ready datasets using R.\n\n\n\nGhozian Islam Karami\n\n\n20 Sep 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploratory Data Analysis for Geological Data with GeoDataViz\n\n\n\nGeoDataViz\n\nEDA\n\nTutorial\n\n\n\nPractical guide to exploratory data analysis for geological datasets using R and GeoDataViz workflows.\n\n\n\nGhozian Islam Karami\n\n\n20 Sep 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to Orbit: Open-Source Geological Analysis Tools\n\n\n\nWelcome\n\nOpen-Source\n\nGeoDataViz\n\n\n\nDiscover Orbit’s suite of R Shiny applications for geological data analysis - from exploration to resource estimation.\n\n\n\nGhozian Islam Karami\n\n\n20 Sep 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nPart 3: Geological Controls & Domain Definition\n\n\n\nEDA\n\nGeological Domains\n\nDomaining\n\nResource Estimation\n\n\n\n\n\n\n\nGhozian Islam Karami\n\n\n29 Jan 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nPart 2: Spatial & Statistical Analysis\n\n\n\nEDA\n\nSpatial Analysis\n\nStatistics\n\nVisualization\n\n\n\n\n\n\n\nGhozian Islam Karami\n\n\n22 Jan 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nPart 1: Foundation & Data Validation\n\n\n\nEDA\n\nData Validation\n\nQAQC\n\nResource Estimation\n\n\n\n\n\n\n\nGhozian Islam Karami\n\n\n15 Jan 2025\n\n\n\n\n\n\nTidak ada yang cocok"
  },
  {
    "objectID": "posts/Data Validation/index.html",
    "href": "posts/Data Validation/index.html",
    "title": "Part 1: Foundation & Data Validation",
    "section": "",
    "text": "Before diving into the technical details, let’s address a fundamental question: Why do mining projects fail?\nThe answer often lies in the quality of the foundational data. Research shows that up to 70% of errors in resource estimation stem from inadequate data validation and EDA processes.\n\n\nGarbage In, Garbage Out (GIGO) - This principle is fundamental to all data analysis work. Poor quality data will always produce poor models, leading to:\n\nInaccurate resource estimates\nFailed mine plans\nInvestor confidence loss\nRegulatory non-compliance\nMillions in losses\n\n\n\n\n\n\n\nPentingIndustry Reality\n\n\n\nIn mining, we don’t get second chances. The quality of our EDA determines whether we build a mine or lose millions in poor decisions."
  },
  {
    "objectID": "posts/Data Validation/index.html#why-eda-is-not-optional",
    "href": "posts/Data Validation/index.html#why-eda-is-not-optional",
    "title": "Part 1: Foundation & Data Validation",
    "section": "",
    "text": "Before diving into the technical details, let’s address a fundamental question: Why do mining projects fail?\nThe answer often lies in the quality of the foundational data. Research shows that up to 70% of errors in resource estimation stem from inadequate data validation and EDA processes.\n\n\nGarbage In, Garbage Out (GIGO) - This principle is fundamental to all data analysis work. Poor quality data will always produce poor models, leading to:\n\nInaccurate resource estimates\nFailed mine plans\nInvestor confidence loss\nRegulatory non-compliance\nMillions in losses\n\n\n\n\n\n\n\nPentingIndustry Reality\n\n\n\nIn mining, we don’t get second chances. The quality of our EDA determines whether we build a mine or lose millions in poor decisions."
  },
  {
    "objectID": "posts/Data Validation/index.html#eda-as-an-industry-standard",
    "href": "posts/Data Validation/index.html#eda-as-an-industry-standard",
    "title": "Part 1: Foundation & Data Validation",
    "section": "EDA as an Industry Standard",
    "text": "EDA as an Industry Standard\nEDA is not just “best practice” - it’s a mandatory requirement for professional resource estimation.\n\nJORC Code Compliance\nThe JORC Code requires that all resource reports be:\n\nTransparent: Methods and data quality must be clearly documented\nMaterial: All relevant information affecting value must be disclosed\nCompetent: Prepared by qualified professionals\n\nEDA directly supports these requirements by:\n\nDocumenting data quality and limitations\nIdentifying material data issues\nProviding evidence for geological interpretations\n\n\n\nCompetent Person Responsibilities\nAs a Competent Person (CP), thorough EDA is part of your due diligence. You are responsible for:\n\nVerifying data integrity\nDocumenting QA/QC procedures\nEnsuring estimation assumptions are data-supported\nDefending your resource model to auditors and regulators"
  },
  {
    "objectID": "posts/Data Validation/index.html#the-4-pillar-eda-framework",
    "href": "posts/Data Validation/index.html#the-4-pillar-eda-framework",
    "title": "Part 1: Foundation & Data Validation",
    "section": "The 4 Pillar EDA Framework",
    "text": "The 4 Pillar EDA Framework\nThis series follows a systematic 4-pillar approach to EDA:\n\n\n\n\n\n\nEach pillar builds upon the previous, creating a comprehensive understanding of your dataset."
  },
  {
    "objectID": "posts/Data Validation/index.html#pillar-1-data-validation-integrity",
    "href": "posts/Data Validation/index.html#pillar-1-data-validation-integrity",
    "title": "Part 1: Foundation & Data Validation",
    "section": "Pillar 1: Data Validation & Integrity",
    "text": "Pillar 1: Data Validation & Integrity\nThe first pillar is the foundation of all subsequent analysis. Without proper data validation, all downstream work becomes meaningless.\n\nWhat We Check\nA comprehensive data validation workflow includes:\n\nFile Integration Checks\n\nCollar file completeness\nAssay file completeness\nLithology file completeness\nCross-file consistency\n\nMissing Data Detection\n\nCollars without assay data\nAssays without collar coordinates\nLithology gaps\n\nInterval Validation\n\nOverlapping intervals\nGaps in sampling\nDepth consistency\n\nGeometric Validation\n\nData above/below topography\nSurvey data quality\nCoordinate system consistency\n\n\n\n\n\n\n\n\nPeringatanCritical Principle\n\n\n\nOne bad data point can invalidate an entire block model if not caught early. Data integrity issues compound through every step of the modeling process."
  },
  {
    "objectID": "posts/Data Validation/index.html#practical-implementation-with-geodataviz",
    "href": "posts/Data Validation/index.html#practical-implementation-with-geodataviz",
    "title": "Part 1: Foundation & Data Validation",
    "section": "Practical Implementation with GeoDataViz",
    "text": "Practical Implementation with GeoDataViz\nLet’s see how these validation checks are implemented using real drilling data.\n\nStep 1: Load Required Libraries\n\n\nKode\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(DT)\nlibrary(plotly)\nlibrary(janitor)\n\n\n\n\nStep 2: Create Sample Data\nFor this demonstration, we’ll create simulated drilling data that mimics real geological scenarios:\n\n\nKode\n# Create simulated collar data\nset.seed(123)\nn_holes &lt;- 50\n\ncollar &lt;- data.frame(\n  hole_id = paste0(\"DDH\", sprintf(\"%03d\", 1:n_holes)),\n  x = runif(n_holes, 500000, 501000),\n  y = runif(n_holes, 9000000, 9001000),\n  rl = runif(n_holes, 100, 200)\n)\n\n# Create simulated assay data (multiple intervals per hole)\nassay_list &lt;- lapply(collar$hole_id, function(hid) {\n  n_intervals &lt;- sample(15:25, 1)\n  depths &lt;- seq(0, by = 2, length.out = n_intervals)\n  \n  data.frame(\n    hole_id = hid,\n    from = depths[-length(depths)],\n    to = depths[-1],\n    au_ppm = pmax(0, rnorm(n_intervals - 1, mean = 1.5, sd = 2)),\n    ag_ppm = pmax(0, rnorm(n_intervals - 1, mean = 15, sd = 20)),\n    cu_pct = pmax(0, rnorm(n_intervals - 1, mean = 0.5, sd = 0.8))\n  )\n})\nassay &lt;- do.call(rbind, assay_list)\n\n# Create simulated lithology data\nlitho_codes &lt;- c(\"Andesite\", \"Diorite\", \"Mineralized_Zone\", \"Altered_Volcanics\")\nlithology_list &lt;- lapply(collar$hole_id, function(hid) {\n  n_litho &lt;- sample(4:8, 1)\n  depths &lt;- sort(c(0, sample(5:40, n_litho - 1), 50))\n  \n  data.frame(\n    hole_id = hid,\n    from = depths[-length(depths)],\n    to = depths[-1],\n    lithology = sample(litho_codes, n_litho, replace = TRUE)\n  )\n})\nlithology &lt;- do.call(rbind, lithology_list)\n\n# Clean names\ncollar &lt;- janitor::clean_names(collar)\nassay &lt;- janitor::clean_names(assay)\nlithology &lt;- janitor::clean_names(lithology)\n\n# Display structure\ncat(\"Collar records:\", nrow(collar), \"\\n\")\n\n\nCollar records: 50 \n\n\nKode\ncat(\"Assay records:\", nrow(assay), \"\\n\")\n\n\nAssay records: 981 \n\n\nKode\ncat(\"Lithology records:\", nrow(lithology), \"\\n\")\n\n\nLithology records: 309 \n\n\n\n\n\n\n\n\nCatatanAbout the Sample Data\n\n\n\nThis is simulated data designed to demonstrate EDA workflows. In practice, you would load your own drilling data from CSV files or databases.\n\n\n\n\nStep 3: File Record Count Validation\nThe first check: do we have data in all files?\n\n\nKode\nfile_counts &lt;- data.frame(\n  File = c(\"Collar\", \"Assay\", \"Lithology\"),\n  Records = c(nrow(collar), nrow(assay), nrow(lithology))\n)\n\ndatatable(file_counts, \n          options = list(dom = 't'),\n          caption = \"Table 1: File Record Counts\")\n\n\n\n\n\n\n\n\n\n\n\n\nTipInterpretation\n\n\n\nAll three files should have records. Empty files indicate data loading issues that must be resolved before proceeding.\n\n\n\n\nStep 4: Cross-File Consistency Checks\n\nCheck 1: Collars Missing Assay Data\n\n\nKode\n# Identify collars without assay data\nmissing_assay &lt;- anti_join(\n  collar %&gt;% distinct(hole_id),\n  assay %&gt;% distinct(hole_id),\n  by = \"hole_id\"\n)\n\nif(nrow(missing_assay) &gt; 0) {\n  datatable(missing_assay,\n            caption = \"Table 2: Collars Missing Assay Data\",\n            options = list(pageLength = 5))\n} else {\n  cat(\"✓ All collars have corresponding assay data.\\n\")\n}\n\n\n✓ All collars have corresponding assay data.\n\n\n\n\nCheck 2: Assays Missing Collar Data\n\n\nKode\n# Identify assays without collar coordinates\nmissing_collar &lt;- anti_join(\n  assay %&gt;% distinct(hole_id),\n  collar %&gt;% distinct(hole_id),\n  by = \"hole_id\"\n)\n\nif(nrow(missing_collar) &gt; 0) {\n  datatable(missing_collar,\n            caption = \"Table 3: Assays Missing Collar Data\",\n            options = list(pageLength = 5))\n} else {\n  cat(\"✓ All assays have corresponding collar coordinates.\\n\")\n}\n\n\n✓ All assays have corresponding collar coordinates.\n\n\n\n\n\n\n\n\nCatatanCommon Causes\n\n\n\nMismatches often result from:\n\nTypos in hole IDs (e.g., “DDH001” vs “DDH-001”)\nIncomplete data transfers\nHoles logged but not yet assayed\nData entry errors\n\n\n\n\n\n\nStep 5: Interval Validation\nOne of the most critical checks: ensuring assay intervals are continuous without gaps or overlaps.\n\n\nKode\n# Check for interval errors (gaps/overlaps)\ninterval_errors &lt;- assay %&gt;%\n  arrange(hole_id, from) %&gt;%\n  group_by(hole_id) %&gt;%\n  mutate(\n    prev_to = lag(to),\n    has_error = !is.na(prev_to) & (from != prev_to)\n  ) %&gt;%\n  ungroup() %&gt;%\n  filter(has_error) %&gt;%\n  select(hole_id, prev_to, from, to)\n\nif(nrow(interval_errors) &gt; 0) {\n  datatable(interval_errors,\n            caption = \"Table 4: Interval Errors (Gaps/Overlaps)\",\n            options = list(pageLength = 10, scrollX = TRUE)) %&gt;%\n    formatStyle('from', backgroundColor = '#ffebee') %&gt;%\n    formatStyle('prev_to', backgroundColor = '#fff9c4')\n} else {\n  cat(\"✓ No interval gaps or overlaps detected.\\n\")\n}\n\n\n✓ No interval gaps or overlaps detected.\n\n\n\n\n\n\n\n\nPentingWhy This Matters\n\n\n\nInterval errors can cause:\n\nIncorrect composite calculations\nGrade dilution or concentration artifacts\nInaccurate tonnage estimates\nBiased variography\n\n\n\n\n\nVisualization: Interval Error Example\n\n\nKode\n# Create example visualization if errors exist\nif(nrow(interval_errors) &gt; 0) {\n  # Take first hole with errors as example\n  example_hole &lt;- interval_errors$hole_id[1]\n  example_data &lt;- assay %&gt;%\n    filter(hole_id == example_hole) %&gt;%\n    arrange(from) %&gt;%\n    head(10)\n  \n  ggplot(example_data, aes(y = from, yend = to)) +\n    geom_segment(aes(x = 0, xend = 1), size = 8, color = \"steelblue\", alpha = 0.7) +\n    geom_text(aes(x = 0.5, y = (from + to)/2, label = paste0(from, \"-\", to)), \n              color = \"white\", fontface = \"bold\", size = 3) +\n    scale_y_reverse() +\n    coord_flip() +\n    labs(\n      title = paste(\"Interval Visualization:\", example_hole),\n      subtitle = \"Look for gaps (white space) or overlaps (segments touching)\",\n      x = NULL,\n      y = \"Depth (m)\"\n    ) +\n    theme_minimal() +\n    theme(\n      axis.text.y = element_blank(),\n      axis.ticks.y = element_blank(),\n      panel.grid.major.y = element_blank()\n    )\n}"
  },
  {
    "objectID": "posts/Data Validation/index.html#data-validation-summary",
    "href": "posts/Data Validation/index.html#data-validation-summary",
    "title": "Part 1: Foundation & Data Validation",
    "section": "Data Validation Summary",
    "text": "Data Validation Summary\n\nKey Metrics Dashboard\n\n\nKode\n# Create validation summary\nvalidation_summary &lt;- data.frame(\n  Check = c(\n    \"Total Collars\",\n    \"Total Assay Intervals\",\n    \"Total Lithology Intervals\",\n    \"Collars Missing Assays\",\n    \"Assays Missing Collars\",\n    \"Interval Errors\"\n  ),\n  Count = c(\n    nrow(collar),\n    nrow(assay),\n    nrow(lithology),\n    nrow(missing_assay),\n    nrow(missing_collar),\n    nrow(interval_errors)\n  ),\n  Status = c(\n    \"✓\", \"✓\", \"✓\",\n    ifelse(nrow(missing_assay) == 0, \"✓\", \"⚠\"),\n    ifelse(nrow(missing_collar) == 0, \"✓\", \"⚠\"),\n    ifelse(nrow(interval_errors) == 0, \"✓\", \"⚠\")\n  )\n)\n\ndatatable(validation_summary,\n          options = list(dom = 't', ordering = FALSE),\n          caption = \"Table 5: Data Validation Summary\",\n          rownames = FALSE) %&gt;%\n  formatStyle(\n    'Status',\n    color = styleEqual(c('✓', '⚠'), c('green', 'orange')),\n    fontWeight = 'bold'\n  )"
  },
  {
    "objectID": "posts/Data Validation/index.html#best-practices-for-data-validation",
    "href": "posts/Data Validation/index.html#best-practices-for-data-validation",
    "title": "Part 1: Foundation & Data Validation",
    "section": "Best Practices for Data Validation",
    "text": "Best Practices for Data Validation\n\nDocumentation Requirements\nFor JORC compliance, document:\n\nData Sources\n\nWho collected the data?\nWhen was it collected?\nWhat QA/QC protocols were followed in the field?\n\nValidation Process\n\nWhat checks were performed?\nWhat issues were found?\nHow were issues resolved?\n\nData Limitations\n\nKnown gaps or uncertainties\nData quality issues that couldn’t be resolved\nImpact on estimation confidence\n\n\n\n\nCommon Pitfalls to Avoid\n\n\n\n\n\n\nPeringatanDon’t Skip These Steps\n\n\n\n\nRushing validation to meet deadlines - Always leads to problems later\nAssuming data is clean - Always validate, even from trusted sources\nFixing issues without documentation - Record all changes for audit trail\nIgnoring “small” errors - Small errors compound in complex workflows"
  },
  {
    "objectID": "posts/Data Validation/index.html#integration-and-data-merging",
    "href": "posts/Data Validation/index.html#integration-and-data-merging",
    "title": "Part 1: Foundation & Data Validation",
    "section": "Integration and Data Merging",
    "text": "Integration and Data Merging\nOnce validation is complete, we can safely merge our datasets:\n\n\nKode\n# Standardize column names\ncollar_std &lt;- collar %&gt;%\n  select(hole_id, x = x, y = y, z = rl) %&gt;%\n  mutate(hole_id = as.character(hole_id))\n\nassay_std &lt;- assay %&gt;%\n  select(hole_id, from, to, everything()) %&gt;%\n  mutate(hole_id = as.character(hole_id))\n\nlithology_std &lt;- lithology %&gt;%\n  select(hole_id, from, to, lithology) %&gt;%\n  mutate(hole_id = as.character(hole_id))\n\n# Merge data\ncombined_data &lt;- assay_std %&gt;%\n  left_join(collar_std, by = \"hole_id\") %&gt;%\n  mutate(mid_point = from + (to - from) / 2) %&gt;%\n  left_join(\n    lithology_std %&gt;% rename(litho_from = from, litho_to = to),\n    by = join_by(hole_id, between(mid_point, litho_from, litho_to))\n  ) %&gt;%\n  select(-mid_point, -litho_from, -litho_to)\n\ncat(\"Combined dataset rows:\", nrow(combined_data), \"\\n\")\n\n\nCombined dataset rows: 1090 \n\n\nKode\ncat(\"Columns:\", paste(names(combined_data), collapse = \", \"), \"\\n\")\n\n\nColumns: hole_id, from, to, au_ppm, ag_ppm, cu_pct, x, y, z, lithology \n\n\n\nPreview Combined Data\n\n\nKode\ndatatable(head(combined_data, 50),\n          options = list(\n            pageLength = 10,\n            scrollX = TRUE,\n            scrollY = \"400px\"\n          ),\n          caption = \"Table 6: Combined Dataset Preview\") %&gt;%\n  formatRound(columns = c('from', 'to', 'x', 'y', 'z'), digits = 2)"
  },
  {
    "objectID": "posts/Data Validation/index.html#checklist-before-moving-to-pillar-2",
    "href": "posts/Data Validation/index.html#checklist-before-moving-to-pillar-2",
    "title": "Part 1: Foundation & Data Validation",
    "section": "Checklist: Before Moving to Pillar 2",
    "text": "Checklist: Before Moving to Pillar 2\nBefore proceeding to spatial analysis, ensure:\n\nAll data files loaded successfully\nNo unexpected missing collars or assays\nInterval errors identified and resolved\nData merge completed without warnings\nValidation results documented\nKnown limitations noted\n\n\n\n\n\n\n\nTipReady for the Next Step?\n\n\n\nWith clean, validated data, you’re ready to explore spatial patterns in Part 2: Spatial & Statistical Analysis."
  },
  {
    "objectID": "posts/Data Validation/index.html#summary",
    "href": "posts/Data Validation/index.html#summary",
    "title": "Part 1: Foundation & Data Validation",
    "section": "Summary",
    "text": "Summary\nData validation is the foundation of reliable resource estimation. Key takeaways:\n\nNever skip validation - It’s mandatory for JORC compliance\nCheck everything - Files, intervals, cross-references\nDocument thoroughly - Create audit trails\nFix issues early - Problems compound downstream\nValidate assumptions - Don’t trust data blindly\n\nRemember the GIGO principle: Quality data is the only path to quality models."
  },
  {
    "objectID": "posts/Data Validation/index.html#tools-and-resources",
    "href": "posts/Data Validation/index.html#tools-and-resources",
    "title": "Part 1: Foundation & Data Validation",
    "section": "Tools and Resources",
    "text": "Tools and Resources\n\nGeoDataViz: GitHub Repository\nJORC Code: 2012 Edition\nContact: ghoziankarami@gmail.com\n\n\nNext: Part 2 - Spatial & Statistical Analysis →"
  },
  {
    "objectID": "posts/EDA with GeoDataViz/index.html",
    "href": "posts/EDA with GeoDataViz/index.html",
    "title": "Exploratory Data Analysis for Geological Data with GeoDataViz",
    "section": "",
    "text": "Inadequate exploratory data analysis is the primary cause of flawed resource models. Before resource estimation, you must understand:\n\nData quality and completeness\nStatistical distributions\nSpatial patterns and bias\nGeological domains\nOutlier populations"
  },
  {
    "objectID": "posts/EDA with GeoDataViz/index.html#why-eda-matters",
    "href": "posts/EDA with GeoDataViz/index.html#why-eda-matters",
    "title": "Exploratory Data Analysis for Geological Data with GeoDataViz",
    "section": "",
    "text": "Inadequate exploratory data analysis is the primary cause of flawed resource models. Before resource estimation, you must understand:\n\nData quality and completeness\nStatistical distributions\nSpatial patterns and bias\nGeological domains\nOutlier populations"
  },
  {
    "objectID": "posts/EDA with GeoDataViz/index.html#the-5-phase-eda-workflow",
    "href": "posts/EDA with GeoDataViz/index.html#the-5-phase-eda-workflow",
    "title": "Exploratory Data Analysis for Geological Data with GeoDataViz",
    "section": "The 5-Phase EDA Workflow",
    "text": "The 5-Phase EDA Workflow\n\nPhase 1: Data Validation\nFirst, check data integrity:\n\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(DT)\n\n# Sample drillhole data\nset.seed(123)\ndrillhole_data &lt;- tibble(\n  HoleID = rep(paste0(\"DH\", 1:10), each = 10),\n  From = rep(seq(0, 90, 10), 10),\n  To = rep(seq(10, 100, 10), 10),\n  Grade = rlnorm(100, meanlog = 0.5, sdlog = 0.8),\n  X = rep(runif(10, 100, 200), each = 10),\n  Y = rep(runif(10, 100, 200), each = 10),\n  Z = rep(seq(50, 95, 5), 10)\n)\n\n# Data summary\ncat(\"Total samples:\", nrow(drillhole_data), \"\\n\")\n\nTotal samples: 100 \n\ncat(\"Unique holes:\", n_distinct(drillhole_data$HoleID), \"\\n\")\n\nUnique holes: 10 \n\ncat(\"Grade range:\", round(min(drillhole_data$Grade), 2), \"-\", \n    round(max(drillhole_data$Grade), 2), \"\\n\")\n\nGrade range: 0.26 - 9.49 \n\n\n\n\nPhase 2: Statistical Analysis\nUnderstand grade distribution:\n\n# Summary statistics\nsummary_stats &lt;- drillhole_data %&gt;%\n  summarise(\n    Mean = mean(Grade),\n    Median = median(Grade),\n    SD = sd(Grade),\n    CV = sd(Grade)/mean(Grade),\n    Skewness = moments::skewness(Grade)\n  )\n\nprint(summary_stats)\n\n# A tibble: 1 × 5\n   Mean Median    SD    CV Skewness\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1  2.31   1.73  1.86 0.805     1.90\n\n# Histogram and boxplot\np1 &lt;- plot_ly(drillhole_data, x = ~Grade, type = \"histogram\", \n              nbinsx = 30, name = \"Distribution\") %&gt;%\n  layout(xaxis = list(title = \"Grade\"), \n         yaxis = list(title = \"Frequency\"))\n\np2 &lt;- plot_ly(drillhole_data, y = ~Grade, type = \"box\", \n              name = \"Boxplot\") %&gt;%\n  layout(yaxis = list(title = \"Grade\"))\n\nsubplot(p1, p2, nrows = 1, shareY = FALSE, titleX = TRUE) %&gt;%\n  layout(title = \"Grade Distribution Analysis\")\n\n\n\n\n\n\n\nGambar 1: Grade distribution analysis\n\n\n\n\nKey Observations: - Check for skewness (common in geological data) - Identify outliers using boxplot - Assess if transformation needed\n\n\nPhase 3: Spatial Analysis\nVisualize spatial patterns:\n\n# Calculate average grade per hole\ncollar_summary &lt;- drillhole_data %&gt;%\n  group_by(HoleID, X, Y) %&gt;%\n  summarise(\n    AvgGrade = mean(Grade),\n    MaxGrade = max(Grade),\n    .groups = 'drop'\n  )\n\n# Interactive map\nplot_ly(collar_summary, x = ~X, y = ~Y, color = ~AvgGrade,\n        type = 'scatter', mode = 'markers',\n        marker = list(size = 15, colorscale = 'Viridis', \n                     showscale = TRUE, colorbar = list(title = \"Grade\")),\n        text = ~paste(\"Hole:\", HoleID, \n                     \"&lt;br&gt;Avg Grade:\", round(AvgGrade, 2))) %&gt;%\n  layout(title = \"Drillhole Collar Map\",\n         xaxis = list(title = \"Easting (X)\"),\n         yaxis = list(title = \"Northing (Y)\"))\n\n\n\n\n\n\n\nGambar 2: Spatial distribution of drillholes\n\n\n\n\nWhat to Look For: - High-grade clustering - Spatial trends - Drilling density variations - Potential bias in sampling\n\n\nPhase 4: Outlier Analysis\nIdentify and understand outliers:\n\n# Calculate IQR method thresholds\nQ1 &lt;- quantile(drillhole_data$Grade, 0.25)\nQ3 &lt;- quantile(drillhole_data$Grade, 0.75)\nIQR &lt;- Q3 - Q1\n\nlower_bound &lt;- Q1 - 1.5 * IQR\nupper_bound &lt;- Q3 + 1.5 * IQR\n\n# Identify outliers\noutliers &lt;- drillhole_data %&gt;%\n  filter(Grade &gt; upper_bound | Grade &lt; lower_bound) %&gt;%\n  arrange(desc(Grade))\n\ncat(\"Number of outliers:\", nrow(outliers), \"\\n\")\n\nNumber of outliers: 8 \n\ncat(\"Outlier threshold:\", round(upper_bound, 2), \"\\n\")\n\nOutlier threshold: 5.5 \n\ncat(\"Top 5 outliers:\\n\")\n\nTop 5 outliers:\n\nprint(head(outliers %&gt;% select(HoleID, From, To, Grade), 5))\n\n# A tibble: 5 × 4\n  HoleID  From    To Grade\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 DH10      60    70  9.49\n2 DH5       30    40  9.35\n3 DH7       90   100  8.50\n4 DH2       50    60  6.89\n5 DH1       50    60  6.50\n\n\nDecision Points: - Are outliers geologically valid? - Do they represent high-grade zones? - Should they be capped or modeled separately?\n\n\nPhase 5: 3D Visualization\nVisualize geological context:\n\n# Create 3D visualization\nplot_ly(drillhole_data, x = ~X, y = ~Y, z = ~Z,\n        color = ~Grade, type = 'scatter3d', mode = 'markers',\n        marker = list(size = 3, colorscale = 'Viridis',\n                     showscale = TRUE, colorbar = list(title = \"Grade\")),\n        text = ~paste(\"Hole:\", HoleID, \n                     \"&lt;br&gt;Depth:\", From, \"-\", To,\n                     \"&lt;br&gt;Grade:\", round(Grade, 2))) %&gt;%\n  layout(title = \"3D Drillhole View\",\n         scene = list(\n           xaxis = list(title = \"Easting\"),\n           yaxis = list(title = \"Northing\"),\n           zaxis = list(title = \"Elevation\", autorange = \"reversed\")\n         ))\n\n\n\n\n\n\n\nGambar 3: 3D drillhole visualization"
  },
  {
    "objectID": "posts/EDA with GeoDataViz/index.html#quick-eda-checklist",
    "href": "posts/EDA with GeoDataViz/index.html#quick-eda-checklist",
    "title": "Exploratory Data Analysis for Geological Data with GeoDataViz",
    "section": "Quick EDA Checklist",
    "text": "Quick EDA Checklist\nBefore proceeding to resource estimation:\n\nData validated (no gaps, coordinates correct)\nStatistical distribution understood\nOutliers identified and assessed\nSpatial patterns reviewed\nGeological domains preliminary defined\nQA/QC results reviewed"
  },
  {
    "objectID": "posts/EDA with GeoDataViz/index.html#using-geodataviz",
    "href": "posts/EDA with GeoDataViz/index.html#using-geodataviz",
    "title": "Exploratory Data Analysis for Geological Data with GeoDataViz",
    "section": "Using GeoDataViz",
    "text": "Using GeoDataViz\nAll these analyses can be performed interactively in GeoDataViz:\n\nUpload Data: CSV/Excel with collar, survey, assay tables\nAuto-Validation: Instant data quality report\nInteractive EDA: Click-and-explore interface\nExport Results: PDF reports and processed data\n\nTry GeoDataViz Demo →"
  },
  {
    "objectID": "posts/EDA with GeoDataViz/index.html#next-steps",
    "href": "posts/EDA with GeoDataViz/index.html#next-steps",
    "title": "Exploratory Data Analysis for Geological Data with GeoDataViz",
    "section": "Next Steps",
    "text": "Next Steps\nAfter completing EDA: 1. Data Preprocessing: Compositing and domain definition 2. Variogram Modeling: Spatial continuity analysis 3. Resource Estimation: Block modeling and kriging\nNext Guide: Pre-Processing Drillhole Data →\n\nFor visual tutorials on these techniques, check @orebit.id on Instagram."
  },
  {
    "objectID": "posts/Spatial Statistics/index.html",
    "href": "posts/Spatial Statistics/index.html",
    "title": "Part 2: Spatial & Statistical Analysis",
    "section": "",
    "text": "With validated data from Part 1, we now explore where the data is located and what it tells us statistically. This post covers:\n\nPillar 2: Spatial Distribution Analysis\nPillar 3: Statistical Characterization\n\nThese pillars help us understand sampling density, identify spatial patterns, and characterize grade distributions - all critical for informed estimation decisions."
  },
  {
    "objectID": "posts/Spatial Statistics/index.html#introduction",
    "href": "posts/Spatial Statistics/index.html#introduction",
    "title": "Part 2: Spatial & Statistical Analysis",
    "section": "",
    "text": "With validated data from Part 1, we now explore where the data is located and what it tells us statistically. This post covers:\n\nPillar 2: Spatial Distribution Analysis\nPillar 3: Statistical Characterization\n\nThese pillars help us understand sampling density, identify spatial patterns, and characterize grade distributions - all critical for informed estimation decisions."
  },
  {
    "objectID": "posts/Spatial Statistics/index.html#setup-and-data-loading",
    "href": "posts/Spatial Statistics/index.html#setup-and-data-loading",
    "title": "Part 2: Spatial & Statistical Analysis",
    "section": "Setup and Data Loading",
    "text": "Setup and Data Loading\n\n\nKode\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(DT)\nlibrary(RColorBrewer)\nlibrary(patchwork)\n\n# Create simulated drilling data\nset.seed(123)\nn_holes &lt;- 50\n\ncollar &lt;- data.frame(\n  hole_id = paste0(\"DDH\", sprintf(\"%03d\", 1:n_holes)),\n  x = runif(n_holes, 500000, 501000),\n  y = runif(n_holes, 9000000, 9001000),\n  rl = runif(n_holes, 100, 200)\n)\n\n# Assay data with spatial correlation\nassay_list &lt;- lapply(1:n_holes, function(i) {\n  n_intervals &lt;- sample(15:25, 1)\n  depths &lt;- seq(0, by = 2, length.out = n_intervals)\n  \n  # Add spatial correlation to grades\n  distance_from_center &lt;- sqrt((collar$x[i] - 500500)^2 + (collar$y[i] - 9000500)^2)\n  grade_factor &lt;- exp(-distance_from_center / 300)\n  \n  data.frame(\n    hole_id = collar$hole_id[i],\n    from = depths[-length(depths)],\n    to = depths[-1],\n    au_ppm = pmax(0, rnorm(n_intervals - 1, mean = 1.5 * grade_factor, sd = 2)),\n    ag_ppm = pmax(0, rnorm(n_intervals - 1, mean = 15 * grade_factor, sd = 20)),\n    cu_pct = pmax(0, rnorm(n_intervals - 1, mean = 0.5 * grade_factor, sd = 0.8))\n  )\n})\nassay &lt;- do.call(rbind, assay_list)\n\n# Lithology data\nlitho_codes &lt;- c(\"Andesite\", \"Diorite\", \"Mineralized_Zone\", \"Altered_Volcanics\")\nlithology_list &lt;- lapply(collar$hole_id, function(hid) {\n  n_litho &lt;- sample(4:8, 1)\n  depths &lt;- sort(c(0, sample(5:40, n_litho - 1), 50))\n  \n  data.frame(\n    hole_id = hid,\n    from = depths[-length(depths)],\n    to = depths[-1],\n    lithology = sample(litho_codes, n_litho, replace = TRUE, \n                      prob = c(0.3, 0.2, 0.3, 0.2))\n  )\n})\nlithology &lt;- do.call(rbind, lithology_list)\n\n# Clean and prepare data\ncollar &lt;- collar %&gt;% janitor::clean_names() %&gt;%\n  select(hole_id, x, y, z = rl) %&gt;%\n  mutate(hole_id = as.character(hole_id))\n\nassay &lt;- assay %&gt;% janitor::clean_names() %&gt;%\n  mutate(hole_id = as.character(hole_id))\n\nlithology &lt;- lithology %&gt;% janitor::clean_names() %&gt;%\n  select(hole_id, from, to, lithology) %&gt;%\n  mutate(hole_id = as.character(hole_id))\n\n# Merge datasets\ncollar_std &lt;- collar\nassay_std &lt;- assay\nlithology_std &lt;- lithology\n\ncombined_data &lt;- assay_std %&gt;%\n  left_join(collar_std, by = \"hole_id\") %&gt;%\n  mutate(mid_point = from + (to - from) / 2) %&gt;%\n  left_join(\n    lithology_std %&gt;% rename(litho_from = from, litho_to = to),\n    by = join_by(hole_id, between(mid_point, litho_from, litho_to))\n  ) %&gt;%\n  select(-mid_point, -litho_from, -litho_to)"
  },
  {
    "objectID": "posts/Spatial Statistics/index.html#objective",
    "href": "posts/Spatial Statistics/index.html#objective",
    "title": "Part 2: Spatial & Statistical Analysis",
    "section": "Objective",
    "text": "Objective\nUnderstand where your data is located in 3D space:\n\nAre drillholes evenly distributed?\nWhere are the dense vs sparse areas?\nWhat are the initial mineralization trends?\nIs there clustering or bias in sampling?\n\n\n\n\n\n\n\nTipWhy Spatial Analysis Matters\n\n\n\nSpatial patterns directly impact:\n\nKriging neighborhood selection\nSearch ellipse orientation\nEstimation variance\nClassification confidence (Measured, Indicated, Inferred)"
  },
  {
    "objectID": "posts/Spatial Statistics/index.html#d-drillhole-location-map",
    "href": "posts/Spatial Statistics/index.html#d-drillhole-location-map",
    "title": "Part 2: Spatial & Statistical Analysis",
    "section": "2D Drillhole Location Map",
    "text": "2D Drillhole Location Map\nLet’s start with a plan view showing collar locations.\n\nBasic Collar Map\n\n\nKode\n# Create 2D plan view\np_2d &lt;- ggplot(collar_std, aes(x = x, y = y)) +\n  geom_point(size = 3, alpha = 0.7, color = \"steelblue\") +\n  geom_text(aes(label = hole_id), \n            hjust = -0.2, vjust = 0.5, \n            size = 2.5, \n            check_overlap = TRUE) +\n  coord_equal() +\n  labs(\n    title = \"2D Drillhole Plan View\",\n    subtitle = paste(nrow(collar_std), \"drillholes\"),\n    x = \"Easting (m)\",\n    y = \"Northing (m)\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    panel.grid.minor = element_blank()\n  )\n\np_2d\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCatatanSpatial Pattern Observations\n\n\n\nLook for:\n\nClustering: Groups of closely spaced holes\nGaps: Areas with no drilling\nGrid pattern: Regular vs irregular spacing\nDirectional bias: Preferential drilling directions\n\n\n\n\n\nInteractive 2D Map with Average Grades\n\n\nKode\n# Calculate average grade per hole (example: using au_ppm)\navg_grades &lt;- combined_data %&gt;%\n  group_by(hole_id, x, y) %&gt;%\n  summarise(\n    avg_au = mean(au_ppm, na.rm = TRUE),\n    max_au = max(au_ppm, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  filter(!is.na(avg_au))\n\n# Create interactive plotly map\nplot_ly(data = avg_grades,\n        x = ~x, y = ~y,\n        type = 'scatter', mode = 'markers',\n        marker = list(\n          size = 10,\n          color = ~avg_au,\n          colorscale = 'Viridis',\n          showscale = TRUE,\n          colorbar = list(title = \"Avg Au&lt;br&gt;(ppm)\")\n        ),\n        text = ~paste0(\n          \"Hole: \", hole_id, \"&lt;br&gt;\",\n          \"Avg Au: \", round(avg_au, 3), \" ppm&lt;br&gt;\",\n          \"Max Au: \", round(max_au, 3), \" ppm\"\n        ),\n        hoverinfo = 'text') %&gt;%\n  layout(\n    title = \"2D Drillhole Map: Average Gold Grades\",\n    xaxis = list(title = \"Easting (m)\"),\n    yaxis = list(\n      title = \"Northing (m)\",\n      scaleanchor = \"x\",\n      scaleratio = 1\n    )\n  )\n\n\n\n\n\n\n\n\nSampling Density Analysis\n\n\nKode\n# Calculate local sampling density\ndensity_estimate &lt;- MASS::kde2d(\n  collar_std$x, \n  collar_std$y, \n  n = 50\n)\n\n# Convert to data frame for ggplot\ndensity_df &lt;- expand.grid(\n  x = density_estimate$x,\n  y = density_estimate$y\n) %&gt;%\n  mutate(density = as.vector(density_estimate$z))\n\np_density &lt;- ggplot() +\n  geom_tile(data = density_df, aes(x = x, y = y, fill = density)) +\n  scale_fill_viridis_c(option = \"plasma\", name = \"Density\") +\n  geom_point(data = collar_std, aes(x = x, y = y), \n             size = 2, color = \"white\", alpha = 0.6) +\n  coord_equal() +\n  labs(\n    title = \"Sampling Density Heatmap\",\n    subtitle = \"Darker areas indicate higher drillhole density\",\n    x = \"Easting (m)\",\n    y = \"Northing (m)\"\n  ) +\n  theme_minimal()\n\np_density\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPentingClassification Implications\n\n\n\nSampling density directly impacts resource classification:\n\nDense areas: Higher confidence → Measured/Indicated\nSparse areas: Lower confidence → Indicated/Inferred\nNo data areas: Extrapolation risk"
  },
  {
    "objectID": "posts/Spatial Statistics/index.html#composite-length-analysis",
    "href": "posts/Spatial Statistics/index.html#composite-length-analysis",
    "title": "Part 2: Spatial & Statistical Analysis",
    "section": "Composite Length Analysis",
    "text": "Composite Length Analysis\nUnderstanding sample support is critical for geostatistics.\n\n\nKode\n# Calculate composite lengths\ncombined_data &lt;- combined_data %&gt;%\n  mutate(composite_length = to - from)\n\n# Summary statistics\nlength_summary &lt;- combined_data %&gt;%\n  summarise(\n    `Min Length` = min(composite_length, na.rm = TRUE),\n    `Q1` = quantile(composite_length, 0.25, na.rm = TRUE),\n    `Median` = median(composite_length, na.rm = TRUE),\n    `Mean` = mean(composite_length, na.rm = TRUE),\n    `Q3` = quantile(composite_length, 0.75, na.rm = TRUE),\n    `Max Length` = max(composite_length, na.rm = TRUE),\n    `Std Dev` = sd(composite_length, na.rm = TRUE)\n  )\n\ndatatable(length_summary,\n          caption = \"Table 1: Composite Length Statistics (meters)\",\n          options = list(dom = 't')) %&gt;%\n  formatRound(columns = 1:7, digits = 2)\n\n\n\n\n\n\nKode\n# Histogram\np_length &lt;- ggplot(combined_data, aes(x = composite_length)) +\n  geom_histogram(bins = 30, fill = \"skyblue\", color = \"black\", alpha = 0.7) +\n  geom_vline(xintercept = mean(combined_data$composite_length, na.rm = TRUE),\n             color = \"red\", linetype = \"dashed\", size = 1) +\n  annotate(\"text\", \n           x = mean(combined_data$composite_length, na.rm = TRUE),\n           y = Inf,\n           label = paste(\"Mean:\", round(mean(combined_data$composite_length, na.rm = TRUE), 2), \"m\"),\n           vjust = 2, color = \"red\", fontface = \"bold\") +\n  labs(\n    title = \"Composite Length Distribution\",\n    subtitle = \"Check for consistency in sample support\",\n    x = \"Composite Length (m)\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal()\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nKode\np_length\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPeringatanVariable Sample Support Issues\n\n\n\nInconsistent sample lengths can cause:\n\nSupport effect: Grade variance differences\nBias in estimation: Longer samples overweighted\nVariogram artifacts: False spatial continuity\n\nSolution: Composite to a consistent length (typically 1m or 2m)"
  },
  {
    "objectID": "posts/Spatial Statistics/index.html#d-visualization",
    "href": "posts/Spatial Statistics/index.html#d-visualization",
    "title": "Part 2: Spatial & Statistical Analysis",
    "section": "3D Visualization",
    "text": "3D Visualization\nUnderstanding the vertical distribution of data is essential.\n\n\nKode\n# Prepare 3D data with depth\ndata_3d &lt;- combined_data %&gt;%\n  mutate(z_sample = z - ((from + to) / 2)) %&gt;%\n  filter(!is.na(x) & !is.na(y) & !is.na(z_sample) & !is.na(au_ppm))\n\n# Sample data for performance (use 50% of data)\nset.seed(123)\ndata_3d_sample &lt;- sample_frac(data_3d, 0.5)\n\n# Create 3D scatter plot\nplot_ly(\n  data = data_3d_sample,\n  x = ~x, y = ~y, z = ~z_sample,\n  type = 'scatter3d', mode = 'markers',\n  marker = list(\n    color = ~au_ppm,\n    colorscale = 'Viridis',\n    colorbar = list(title = \"Au (ppm)\"),\n    size = 3\n  ),\n  text = ~paste0(\n    \"Hole: \", hole_id, \"&lt;br&gt;\",\n    \"Depth: \", round(from, 1), \"-\", round(to, 1), \"m&lt;br&gt;\",\n    \"Au: \", round(au_ppm, 3), \" ppm\"\n  ),\n  hoverinfo = 'text'\n) %&gt;%\n  layout(\n    title = \"3D Drillhole Assay View\",\n    scene = list(\n      xaxis = list(title = \"Easting\"),\n      yaxis = list(title = \"Northing\"),\n      zaxis = list(title = \"Elevation (RL)\", autorange = \"reversed\"),\n      aspectratio = list(x = 1, y = 1, z = 0.5)\n    )\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nTip3D Visualization Benefits\n\n\n\n3D views help identify:\n\nVertical trends in mineralization\nDip and plunge of ore bodies\nDrilling depth coverage\nSpatial clustering in 3D space"
  },
  {
    "objectID": "posts/Spatial Statistics/index.html#objective-1",
    "href": "posts/Spatial Statistics/index.html#objective-1",
    "title": "Part 2: Spatial & Statistical Analysis",
    "section": "Objective",
    "text": "Objective\nUnderstand the “personality” of your grade data:\n\nWhat does the distribution look like?\nAre there outliers?\nSingle or multiple populations?\nWhat top-cut value is appropriate?"
  },
  {
    "objectID": "posts/Spatial Statistics/index.html#univariate-statistics",
    "href": "posts/Spatial Statistics/index.html#univariate-statistics",
    "title": "Part 2: Spatial & Statistical Analysis",
    "section": "Univariate Statistics",
    "text": "Univariate Statistics\n\nSummary Statistics by Grade Variable\n\n\nKode\n# Select numeric grade columns\ngrade_cols &lt;- c(\"au_ppm\", \"ag_ppm\", \"cu_pct\")\n\n# Calculate comprehensive statistics\nstats_summary &lt;- combined_data %&gt;%\n  select(all_of(grade_cols)) %&gt;%\n  pivot_longer(everything(), names_to = \"Grade\", values_to = \"Value\") %&gt;%\n  group_by(Grade) %&gt;%\n  summarise(\n    Count = sum(!is.na(Value)),\n    Mean = mean(Value, na.rm = TRUE),\n    Median = median(Value, na.rm = TRUE),\n    `Std Dev` = sd(Value, na.rm = TRUE),\n    CV = sd(Value, na.rm = TRUE) / mean(Value, na.rm = TRUE),\n    Min = min(Value, na.rm = TRUE),\n    Q1 = quantile(Value, 0.25, na.rm = TRUE),\n    Q3 = quantile(Value, 0.75, na.rm = TRUE),\n    Max = max(Value, na.rm = TRUE),\n    Skewness = e1071::skewness(Value, na.rm = TRUE)\n  ) %&gt;%\n  mutate(across(where(is.numeric), ~round(.x, 3)))\n\ndatatable(stats_summary,\n          caption = \"Table 2: Comprehensive Grade Statistics\",\n          options = list(scrollX = TRUE, pageLength = 10))\n\n\n\n\n\n\n\n\n\n\n\n\nCatatanKey Statistical Indicators\n\n\n\n\nCV (Coefficient of Variation): Measure of variability (CV &gt; 1 indicates high variability)\nSkewness: Positive skew common in grade data (long right tail)\nMean vs Median: Large differences suggest outliers"
  },
  {
    "objectID": "posts/Spatial Statistics/index.html#grade-distribution-analysis",
    "href": "posts/Spatial Statistics/index.html#grade-distribution-analysis",
    "title": "Part 2: Spatial & Statistical Analysis",
    "section": "Grade Distribution Analysis",
    "text": "Grade Distribution Analysis\n\nHistograms\n\n\nKode\n# Create histograms for each grade\nplot_list &lt;- lapply(grade_cols, function(col) {\n  ggplot(combined_data, aes(x = .data[[col]])) +\n    geom_histogram(bins = 50, fill = \"skyblue\", color = \"black\", alpha = 0.7) +\n    geom_vline(xintercept = mean(combined_data[[col]], na.rm = TRUE),\n               color = \"red\", linetype = \"dashed\", size = 1) +\n    geom_vline(xintercept = median(combined_data[[col]], na.rm = TRUE),\n               color = \"blue\", linetype = \"dashed\", size = 1) +\n    labs(\n      title = paste(\"Distribution of\", col),\n      x = col,\n      y = \"Frequency\"\n    ) +\n    theme_minimal() +\n    theme(plot.title = element_text(size = 10))\n})\n\n# Combine plots\n(plot_list[[1]] | plot_list[[2]]) / plot_list[[3]]\n\n\n\n\n\n\n\n\n\n\n\nLog-Normal Probability Plots\nEssential for identifying populations and assessing normality.\n\n\nKode\n# Create Q-Q plots for log-transformed data\nqq_plots &lt;- lapply(grade_cols, function(col) {\n  data_subset &lt;- combined_data %&gt;%\n    filter(!is.na(.data[[col]]) & .data[[col]] &gt; 0)\n  \n  ggplot(data_subset, aes(sample = log(.data[[col]]))) +\n    stat_qq(color = \"steelblue\", alpha = 0.5) +\n    stat_qq_line(color = \"red\", size = 1) +\n    labs(\n      title = paste(\"Log-Normal Q-Q Plot:\", col),\n      x = \"Theoretical Quantiles\",\n      y = \"Sample Quantiles (log scale)\"\n    ) +\n    theme_minimal()\n})\n\n(qq_plots[[1]] | qq_plots[[2]]) / qq_plots[[3]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPentingPopulation Identification\n\n\n\nBreaks or changes in slope on Q-Q plots indicate:\n\nMultiple populations: Different geological/mineralization domains\nOutliers: High-grade samples requiring investigation\nMixed distributions: Need for domain separation\n\nAction: Use this information for domaining decisions!"
  },
  {
    "objectID": "posts/Spatial Statistics/index.html#outlier-detection-and-analysis",
    "href": "posts/Spatial Statistics/index.html#outlier-detection-and-analysis",
    "title": "Part 2: Spatial & Statistical Analysis",
    "section": "Outlier Detection and Analysis",
    "text": "Outlier Detection and Analysis\n\nBoxplots by Lithology\n\n\nKode\n# Focus on Au for outlier analysis\np_box &lt;- ggplot(combined_data, aes(x = lithology, y = au_ppm, fill = lithology)) +\n  geom_boxplot(outlier.colour = \"red\", outlier.shape = 16, outlier.size = 2) +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(\n    title = \"Gold Grade Distribution by Lithology\",\n    subtitle = \"Red points indicate statistical outliers (&gt;1.5 IQR)\",\n    x = \"Lithology\",\n    y = \"Au (ppm)\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.position = \"none\"\n  )\n\np_box\n\n\n\n\n\n\n\n\n\n\n\nOutlier Table\n\n\nKode\n# Calculate IQR-based outliers\noutlier_threshold &lt;- combined_data %&gt;%\n  summarise(\n    Q1 = quantile(au_ppm, 0.25, na.rm = TRUE),\n    Q3 = quantile(au_ppm, 0.75, na.rm = TRUE)\n  ) %&gt;%\n  mutate(\n    IQR = Q3 - Q1,\n    lower_bound = Q1 - 1.5 * IQR,\n    upper_bound = Q3 + 1.5 * IQR\n  )\n\noutliers &lt;- combined_data %&gt;%\n  filter(au_ppm &gt; outlier_threshold$upper_bound | \n         au_ppm &lt; outlier_threshold$lower_bound) %&gt;%\n  select(hole_id, from, to, lithology, au_ppm) %&gt;%\n  arrange(desc(au_ppm))\n\ndatatable(outliers,\n          caption = paste(\"Table 3: Outlier Samples (n =\", nrow(outliers), \")\"),\n          options = list(pageLength = 10, scrollX = TRUE)) %&gt;%\n  formatStyle('au_ppm', backgroundColor = '#ffebee', fontWeight = 'bold')\n\n\n\n\n\n\n\n\n\n\n\n\nPeringatanOutlier Treatment Options\n\n\n\n\nKeep as-is: If geologically justified\nTop-cut (cap): Limit maximum grade\nRemove: If analytical errors suspected\nInvestigate: Check for sample preparation issues\n\nNever remove outliers without geological justification!"
  },
  {
    "objectID": "posts/Spatial Statistics/index.html#top-cut-analysis",
    "href": "posts/Spatial Statistics/index.html#top-cut-analysis",
    "title": "Part 2: Spatial & Statistical Analysis",
    "section": "Top-Cut Analysis",
    "text": "Top-Cut Analysis\nDetermining appropriate grade capping values.\n\n\nKode\n# Calculate percentiles\npercentiles &lt;- seq(0.9, 1.0, by = 0.01)\npercentile_values &lt;- quantile(combined_data$au_ppm, probs = percentiles, na.rm = TRUE)\n\npercentile_df &lt;- data.frame(\n  Percentile = percentiles * 100,\n  Grade = percentile_values\n)\n\n# Plot grade vs percentile\np_percentile &lt;- ggplot(percentile_df, aes(x = Percentile, y = Grade)) +\n  geom_line(color = \"steelblue\", size = 1.5) +\n  geom_point(size = 2, color = \"darkblue\") +\n  geom_vline(xintercept = c(95, 97.5, 99), \n             linetype = \"dashed\", \n             color = c(\"orange\", \"red\", \"darkred\")) +\n  annotate(\"text\", x = 95, y = max(percentile_values) * 0.9, \n           label = \"P95\", color = \"orange\") +\n  annotate(\"text\", x = 97.5, y = max(percentile_values) * 0.9, \n           label = \"P97.5\", color = \"red\") +\n  annotate(\"text\", x = 99, y = max(percentile_values) * 0.9, \n           label = \"P99\", color = \"darkred\") +\n  labs(\n    title = \"Top-Cut Analysis: Grade vs Percentile\",\n    subtitle = \"Common thresholds: P95, P97.5, P99\",\n    x = \"Percentile (%)\",\n    y = \"Au Grade (ppm)\"\n  ) +\n  theme_minimal()\n\np_percentile\n\n\n\n\n\n\n\n\n\n\nImpact of Top-Cutting\n\n\nKode\n# Compare statistics with different top-cuts\ntop_cuts &lt;- c(Inf, \n              quantile(combined_data$au_ppm, 0.95, na.rm = TRUE),\n              quantile(combined_data$au_ppm, 0.975, na.rm = TRUE),\n              quantile(combined_data$au_ppm, 0.99, na.rm = TRUE))\n\nimpact_df &lt;- data.frame(\n  Scenario = c(\"No Top-Cut\", \"P95 Cut\", \"P97.5 Cut\", \"P99 Cut\"),\n  `Cut Value` = round(top_cuts, 2),\n  Mean = sapply(top_cuts, function(tc) {\n    mean(pmin(combined_data$au_ppm, tc), na.rm = TRUE)\n  }),\n  Median = sapply(top_cuts, function(tc) {\n    median(pmin(combined_data$au_ppm, tc), na.rm = TRUE)\n  }),\n  `Std Dev` = sapply(top_cuts, function(tc) {\n    sd(pmin(combined_data$au_ppm, tc), na.rm = TRUE)\n  }),\n  CV = sapply(top_cuts, function(tc) {\n    sd(pmin(combined_data$au_ppm, tc), na.rm = TRUE) / \n      mean(pmin(combined_data$au_ppm, tc), na.rm = TRUE)\n  })\n) %&gt;%\n  mutate(across(where(is.numeric), ~round(.x, 3)))\n\ndatatable(impact_df,\n          caption = \"Table 4: Impact of Top-Cutting on Statistics\",\n          options = list(dom = 't'))\n\n\n\n\n\n\n\n\n\n\n\n\nTipSelecting Top-Cut Values\n\n\n\nConsider:\n\nStatistical analysis: Probability plots, percentiles\nGeological context: Are high grades real or analytical errors?\nImpact on resources: Balance between grade and tonnage\nIndustry standards: P95-P99 common for precious metals\nDomaining: Different top-cuts for different domains\n\nDocument your decision with geological and statistical justification!"
  },
  {
    "objectID": "posts/Spatial Statistics/index.html#grade-distribution-by-lithology",
    "href": "posts/Spatial Statistics/index.html#grade-distribution-by-lithology",
    "title": "Part 2: Spatial & Statistical Analysis",
    "section": "Grade Distribution by Lithology",
    "text": "Grade Distribution by Lithology\nUnderstanding how grades vary by rock type.\n\n\nKode\n# Faceted histograms by lithology\nggplot(combined_data, aes(x = au_ppm)) +\n  geom_histogram(bins = 30, fill = \"skyblue\", color = \"black\", alpha = 0.7) +\n  facet_wrap(~ lithology, scales = \"free_y\", ncol = 2) +\n  labs(\n    title = \"Gold Grade Distribution by Lithology\",\n    x = \"Au (ppm)\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal() +\n  theme(strip.text = element_text(face = \"bold\", size = 10))\n\n\n\n\n\n\n\n\n\n\nStatistical Comparison by Lithology\n\n\nKode\nlitho_stats &lt;- combined_data %&gt;%\n  group_by(lithology) %&gt;%\n  summarise(\n    Count = n(),\n    Mean = mean(au_ppm, na.rm = TRUE),\n    Median = median(au_ppm, na.rm = TRUE),\n    `Std Dev` = sd(au_ppm, na.rm = TRUE),\n    CV = sd(au_ppm, na.rm = TRUE) / mean(au_ppm, na.rm = TRUE),\n    Max = max(au_ppm, na.rm = TRUE)\n  ) %&gt;%\n  arrange(desc(Mean)) %&gt;%\n  mutate(across(where(is.numeric) & !Count, ~round(.x, 3)))\n\ndatatable(litho_stats,\n          caption = \"Table 5: Grade Statistics by Lithology\",\n          options = list(pageLength = 10)) %&gt;%\n  formatStyle(\n    'Mean',\n    background = styleColorBar(litho_stats$Mean, 'lightblue'),\n    backgroundSize = '100% 90%',\n    backgroundRepeat = 'no-repeat',\n    backgroundPosition = 'center'\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nCatatanGeological Insights\n\n\n\nThis analysis reveals:\n\nHost rocks: Which lithologies contain mineralization?\nBarren zones: Low-grade lithologies to exclude?\nGrade differences: Justification for separate domains?\nContinuity: Are grades consistent within lithology?"
  },
  {
    "objectID": "posts/Spatial Statistics/index.html#summary-pillars-2-3",
    "href": "posts/Spatial Statistics/index.html#summary-pillars-2-3",
    "title": "Part 2: Spatial & Statistical Analysis",
    "section": "Summary: Pillars 2 & 3",
    "text": "Summary: Pillars 2 & 3\n\nKey Achievements\nSpatial Analysis (Pillar 2): ✓ Visualized drillhole distribution in 2D and 3D ✓ Identified sampling density patterns ✓ Analyzed composite length consistency ✓ Recognized spatial trends in mineralization\nStatistical Analysis (Pillar 3): ✓ Characterized grade distributions ✓ Identified outliers and proposed top-cuts ✓ Compared statistics by lithology ✓ Assessed data populations\n\n\nChecklist: Before Moving to Pillar 4\n\nSpatial patterns understood and documented\nSampling density adequate for classification\nGrade populations identified\nOutliers investigated and treatment decided\nTop-cut values proposed with justification\nLithology-grade relationships documented\n\n\n\n\n\n\n\nTipReady for Geological Controls?\n\n\n\nWith spatial and statistical understanding complete, you’re ready to connect these patterns to geology in Part 3: Geological Controls & Domain Definition."
  },
  {
    "objectID": "posts/Spatial Statistics/index.html#best-practices-recap",
    "href": "posts/Spatial Statistics/index.html#best-practices-recap",
    "title": "Part 2: Spatial & Statistical Analysis",
    "section": "Best Practices Recap",
    "text": "Best Practices Recap\n\nCommon Pitfalls to Avoid\n\nIgnoring spatial patterns - Statistics alone aren’t enough\nOver-relying on automation - Always apply geological thinking\nArbitrary top-cutting - Document geological justification\nSkipping lithology analysis - Rock types control grades\n\n\n\nDocumentation Requirements\nFor JORC compliance, document:\n\nSpatial distribution maps and density analysis\nStatistical summary by domain/lithology\nOutlier treatment decisions with justification\nTop-cut analysis and selected values\nPopulation identification methodology\n\n\nPrevious: ← Part 1 - Data Validation\nNext: Part 3 - Geological Controls & Domain Definition →"
  }
]